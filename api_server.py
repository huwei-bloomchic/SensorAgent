#!/usr/bin/env python3
"""
ç¥ç­–æ•°æ®åˆ†æåŠ©æ‰‹ - OpenAPIæœåŠ¡å™¨
æä¾›å…¼å®¹OpenAIæ ¼å¼çš„ /v1/chat/completions æ¥å£
"""
import sys
import json
import asyncio
import uuid
from pathlib import Path
from typing import Optional, List, Dict, Any, AsyncGenerator
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from loguru import logger
import os

from config.settings import get_settings
from src.agents.orchestrator_v2 import create_agent_v2


# ============ Pydanticæ¨¡å‹å®šä¹‰ ============

class Message(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(..., description="è§’è‰²: system/user/assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatCompletionRequest(BaseModel):
    """Chat Completionè¯·æ±‚"""
    model: str = Field(default="sensors-agent", description="æ¨¡å‹åç§°")
    messages: List[Message] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¿”å›")
    temperature: Optional[float] = Field(default=0.7, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(default=None, description="æœ€å¤§tokenæ•°")
    top_p: Optional[float] = Field(default=1.0, description="Top-pé‡‡æ ·")


class ChatCompletionChoice(BaseModel):
    """Chat Completioné€‰æ‹©"""
    index: int
    message: Message
    finish_reason: str


class Usage(BaseModel):
    """Tokenä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0


class ChatCompletionResponse(BaseModel):
    """Chat Completionå“åº”"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage


class DeltaMessage(BaseModel):
    """æµå¼å“åº”çš„å¢é‡æ¶ˆæ¯"""
    role: Optional[str] = None
    content: Optional[str] = None


class ChatCompletionStreamChoice(BaseModel):
    """æµå¼å“åº”çš„é€‰æ‹©"""
    index: int
    delta: DeltaMessage
    finish_reason: Optional[str] = None


class ChatCompletionStreamResponse(BaseModel):
    """æµå¼å“åº”"""
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[ChatCompletionStreamChoice]


# ============ FastAPIåº”ç”¨ ============

app = FastAPI(
    title="ç¥ç­–æ•°æ®åˆ†æåŠ©æ‰‹ API",
    description="æä¾›å…¼å®¹OpenAIæ ¼å¼çš„èŠå¤©APIï¼Œæ”¯æŒç¥ç­–æ•°æ®åˆ†æ",
    version="2.0.0"
)

# æ·»åŠ CORSæ”¯æŒ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€Agentå®ä¾‹
agent = None
settings = None


# ============ AgentåŒ…è£…å™¨ ============

class StreamingAgentWrapper:
    """
    AgentåŒ…è£…å™¨ï¼Œæ”¯æŒæµå¼è¿”å›thinkingæ­¥éª¤å’Œæœ€ç»ˆç»“æœ
    """

    def __init__(self, agent):
        self.agent = agent

    async def query_streaming(
        self,
        user_input: str,
        task_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æ‰§è¡ŒæŸ¥è¯¢ï¼Œyieldä¸­é—´æ­¥éª¤å’Œæœ€ç»ˆç»“æœ
        åŸºäºTaskContextç”Ÿæˆå®Œæ•´çš„Markdownæ ¼å¼thinkingè¾“å‡º

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢
            task_id: ä»»åŠ¡IDï¼Œç”¨äºCSVæ–‡ä»¶å‘½å

        Yields:
            {"type": "thinking", "content": "æ€è€ƒæ­¥éª¤å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰"}
            {"type": "answer", "content": "æœ€ç»ˆç­”æ¡ˆ"}
        """
        from src.models.task_context import TaskContext
        
        try:
            # å¦‚æœæ²¡æœ‰æä¾›task_idï¼Œç”Ÿæˆä¸€ä¸ª
            if not task_id:
                task_id = uuid.uuid4().hex[:8]
            
            # åˆ›å»ºTaskContext
            task_context = TaskContext(
                task_id=task_id,
                user_question=user_input
            )
            
            # å‘é€å¼€å§‹æ€è€ƒä¿¡å·ï¼ˆåŸºäºTaskContextï¼‰
            progress_md = self._format_context_progress_markdown(task_context)
            yield {
                "type": "thinking",
                "content": progress_md
            }
            await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒï¼Œç¡®ä¿ç«‹å³å‘é€

            # é˜¶æ®µ1: åˆæ­¥åˆ†æ
            task_context.start_iteration(
                iteration_type="initial",
                name="åˆæ­¥åˆ†æ",
                description="ç†è§£ç”¨æˆ·é—®é¢˜å¹¶åˆ¶å®šåˆ†æè®¡åˆ’"
            )
            
            progress_md = self._format_context_progress_markdown(task_context)
            yield {
                "type": "thinking",
                "content": progress_md
            }
            await asyncio.sleep(0)

            # åœ¨çº¿ç¨‹æ± ä¸­è°ƒç”¨åŒæ­¥æ–¹æ³•
            loop = asyncio.get_event_loop()
            analysis_result = await loop.run_in_executor(
                None,
                self.agent.analyst_agent.analyze,
                user_input,
                "initial"
            )
            analysis_plan = analysis_result.get("analysis_plan", "")

            # è§£ææŒ‡ä»¤
            instructions = self.agent._parse_instructions(analysis_plan)

            if not instructions:
                instructions = [{
                    "task": user_input,
                    "time_range": "last_7_days"
                }]

            # æ›´æ–°TaskContextå¹¶ç”Ÿæˆthinkingè¾“å‡º
            progress_md = self._format_context_progress_markdown(task_context)
            yield {
                "type": "thinking",
                "content": progress_md
            }
            await asyncio.sleep(0)

            # é˜¶æ®µ2: æ‰§è¡Œåˆæ­¥æŸ¥è¯¢
            # ä½¿ç”¨agentçš„_execute_instructionsæ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨æ›´æ–°TaskContext
            initial_results = await loop.run_in_executor(
                None,
                self.agent._execute_instructions,
                instructions,
                task_id,
                task_context
            )
            
            # å®Œæˆåˆå§‹è¿­ä»£
            task_context.complete_iteration()
            
            # ç”ŸæˆåŸºäºTaskContextçš„thinkingè¾“å‡º
            progress_md = self._format_context_progress_markdown(task_context)
            yield {
                "type": "thinking",
                "content": progress_md
            }
            await asyncio.sleep(0)

            # é˜¶æ®µ3: è¯„ä¼°æ˜¯å¦éœ€è¦ä¸‹é’»
            success_count = sum(1 for r in initial_results if r.get("status") == "success")

            drilldown_results = []
            drilldown_instructions = []

            if success_count > 0:
                # æ›´æ–°thinkingè¾“å‡º
                progress_md = self._format_context_progress_markdown(task_context)
                yield {
                    "type": "thinking",
                    "content": progress_md
                }
                await asyncio.sleep(0)

                decision = await loop.run_in_executor(
                    None,
                    self.agent.analyst_agent.evaluate_and_decide_drilldown,
                    user_input,
                    initial_results
                )

                if decision["need_drilldown"]:
                    # å¼€å§‹ä¸‹é’»è¿­ä»£
                    task_context.start_iteration(
                        iteration_type="drilldown",
                        name="æ·±å…¥åˆ†æ",
                        description=f"åŸºäºåˆæ­¥ç»“æœè¿›è¡Œæ·±å…¥åˆ†æã€‚ç†ç”±: {decision['reasoning']}"
                    )
                    
                    # æ›´æ–°thinkingè¾“å‡º
                    progress_md = self._format_context_progress_markdown(task_context)
                    yield {
                        "type": "thinking",
                        "content": progress_md
                    }
                    await asyncio.sleep(0)

                    # ç”Ÿæˆä¸‹é’»æŒ‡ä»¤
                    context = {
                        "initial_results": self.agent.analyst_agent._extract_results_summary(initial_results),
                        "suggested_dimensions": decision["suggested_dimensions"]
                    }

                    def analyze_drilldown():
                        return self.agent.analyst_agent.analyze(
                            user_question=user_input,
                            context=context,
                            stage="drilldown"
                        )

                    drilldown_analysis = await loop.run_in_executor(None, analyze_drilldown)
                    drilldown_plan = drilldown_analysis.get("analysis_plan", "")
                    drilldown_instructions = self.agent._parse_instructions(drilldown_plan)

                    if drilldown_instructions:
                        # æ‰§è¡Œä¸‹é’»æŸ¥è¯¢
                        drilldown_results = await loop.run_in_executor(
                            None,
                            self.agent._execute_instructions,
                            drilldown_instructions,
                            task_id,
                            task_context
                        )
                        
                        # å®Œæˆä¸‹é’»è¿­ä»£
                        task_context.complete_iteration()
                        
                        # æ›´æ–°thinkingè¾“å‡º
                        progress_md = self._format_context_progress_markdown(task_context)
                        yield {
                            "type": "thinking",
                            "content": progress_md
                        }
                        await asyncio.sleep(0)
                else:
                    # æ›´æ–°thinkingè¾“å‡º
                    progress_md = self._format_context_progress_markdown(task_context)
                    yield {
                        "type": "thinking",
                        "content": progress_md
                    }
                    await asyncio.sleep(0)

            # é˜¶æ®µ4: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            all_results = initial_results + drilldown_results
            all_instructions = instructions + drilldown_instructions

            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            task_context.completed_at = datetime.now()
            
            # ç”Ÿæˆæœ€ç»ˆçš„thinkingè¾“å‡º
            progress_md = self._format_context_progress_markdown(task_context)
            yield {
                "type": "thinking",
                "content": progress_md
            }
            await asyncio.sleep(0)

            # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            if len(all_results) == 1 and all_results[0].get("status") == "success" and not drilldown_results:
                yield {
                    "type": "thinking",
                    "content": "âœï¸ ç”Ÿæˆå•ä¸€æŸ¥è¯¢ç»“æœæŠ¥å‘Š...\n"
                }
                await asyncio.sleep(0)

                from src.utils.report_formatter import ReportFormatter
                final_answer = await loop.run_in_executor(
                    None,
                    ReportFormatter.format_single_result,
                    user_input,
                    all_results[0]
                )
            else:
                yield {
                    "type": "thinking",
                    "content": f"âœï¸ ç»¼åˆåˆ†æ {len(all_results)} ä¸ªæŸ¥è¯¢ç»“æœ...\n"
                }
                await asyncio.sleep(0)

                def generate_final():
                    from src.utils.report_formatter import ReportFormatter
                    synthesis_report = self.agent.analyst_agent.synthesize_results(
                        instructions=all_instructions,
                        results=all_results
                    )
                    return ReportFormatter.format_multiple_results(
                        user_question=user_input,
                        analysis_plan=analysis_plan,
                        initial_results=initial_results,
                        drilldown_results=drilldown_results,
                        synthesis_report=synthesis_report,
                        extract_plan_summary=self.agent._extract_plan_summary
                    )

                final_answer = await loop.run_in_executor(None, generate_final)

            yield {
                "type": "thinking",
                "content": "âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ\n"
            }
            await asyncio.sleep(0)

            # å‘é€æœ€ç»ˆç­”æ¡ˆ
            yield {
                "type": "answer",
                "content": final_answer
            }

        except Exception as e:
            logger.exception("æµå¼æŸ¥è¯¢å¤„ç†å¤±è´¥")
            yield {
                "type": "error",
                "content": f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            }

    def _extract_plan_summary(self, analysis_plan: str) -> str:
        """æå–åˆ†æè®¡åˆ’æ‘˜è¦"""
        lines = analysis_plan.split('\n')
        summary_lines = []

        for line in lines:
            line = line.strip()
            if line.startswith('```') or line.startswith('{') or line.startswith('['):
                continue
            if line and not line.startswith('#'):
                summary_lines.append(line)
                if len(summary_lines) >= 3:
                    break

        return '\n'.join(summary_lines) if summary_lines else "åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”ŸæˆæŸ¥è¯¢è®¡åˆ’"

    def _format_context_progress_markdown(self, task_context) -> str:
        """
        åŸºäºTaskContextç”ŸæˆMarkdownæ ¼å¼çš„è¿›åº¦æŠ¥å‘Š
        
        Args:
            task_context: TaskContextå¯¹è±¡
            
        Returns:
            Markdownæ ¼å¼çš„è¿›åº¦æŠ¥å‘Š
        """
        from src.models.task_context import TaskContext
        
        if not isinstance(task_context, TaskContext):
            return ""
        
        lines = []
        
        # ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
        lines.append("## ğŸ“‹ ä»»åŠ¡æ‰§è¡Œè¿›åº¦")
        lines.append("")
        lines.append(f"**ä»»åŠ¡ID:** `{task_context.task_id}`")
        lines.append(f"**ç”¨æˆ·é—®é¢˜:** {task_context.user_question}")
        lines.append(f"**çŠ¶æ€:** {'âœ… å·²å®Œæˆ' if task_context.completed_at else 'ğŸ”„ æ‰§è¡Œä¸­'}")
        lines.append("")
        
        # è¿­ä»£è¿›åº¦
        if task_context.iterations:
            lines.append("### ğŸ”„ æ‰§è¡Œé˜¶æ®µ")
            lines.append("")
            
            for iteration in task_context.iterations:
                # è¿­ä»£çŠ¶æ€
                status_icon = "âœ…" if iteration.completed_at else "ğŸ”„"
                lines.append(f"#### {status_icon} {iteration.name}")
                lines.append("")
                
                if iteration.description:
                    lines.append(f"*{iteration.description}*")
                    lines.append("")
                
                # è¿­ä»£ç»Ÿè®¡
                total_queries = len(iteration.queries)
                successful_queries = sum(1 for q in iteration.queries if q.status == "success")
                failed_queries = sum(1 for q in iteration.queries if q.status == "failed")
                cached_queries = sum(1 for q in iteration.queries if q.from_cache)
                
                lines.append(f"- **æŸ¥è¯¢æ€»æ•°:** {total_queries}")
                lines.append(f"- **æˆåŠŸ:** {successful_queries} âœ…")
                if failed_queries > 0:
                    lines.append(f"- **å¤±è´¥:** {failed_queries} âŒ")
                if cached_queries > 0:
                    lines.append(f"- **ç¼“å­˜:** {cached_queries} âš¡")
                lines.append("")
                
                # æŸ¥è¯¢è¯¦æƒ…
                if iteration.queries:
                    lines.append("**æŸ¥è¯¢è¯¦æƒ…:**")
                    lines.append("")
                    
                    for query in iteration.queries:
                        query_status_icon = {
                            "success": "âœ…",
                            "failed": "âŒ",
                            "partial": "âš ï¸",
                            "pending": "â³"
                        }.get(query.status, "â“")
                        
                        cache_mark = " (ç¼“å­˜)" if query.from_cache else ""
                        lines.append(f"{query_status_icon} **æŸ¥è¯¢ {query.query_sequence}:** {query.instruction[:100]}{'...' if len(query.instruction) > 100 else ''}{cache_mark}")
                        
                        # SQLä¿¡æ¯
                        if query.sql:
                            lines.append(f"  - **SQL:** å·²ç”Ÿæˆ")
                            if query.sql_execution_time_ms:
                                lines.append(f"  - **æ‰§è¡Œæ—¶é—´:** {query.sql_execution_time_ms}ms")
                        
                        # æ•°æ®ä¿¡æ¯
                        if query.csv_path:
                            lines.append(f"  - **æ•°æ®æ–‡ä»¶:** `{query.csv_path.split('/')[-1]}`")
                            if query.data_result_row_count > 0:
                                lines.append(f"  - **æ•°æ®è¡Œæ•°:** {query.data_result_row_count:,} è¡Œ")
                            if query.data_result_column_count:
                                lines.append(f"  - **åˆ—æ•°:** {query.data_result_column_count} åˆ—")
                        
                        # é”™è¯¯ä¿¡æ¯
                        if query.status == "failed" and query.error:
                            lines.append(f"  - **é”™è¯¯:** {query.error[:200]}")
                        
                        lines.append("")
                
                # è¿­ä»£å®Œæˆæ—¶é—´
                if iteration.completed_at:
                    duration = (iteration.completed_at - iteration.started_at).total_seconds()
                    lines.append(f"*å®Œæˆæ—¶é—´: {iteration.completed_at.strftime('%H:%M:%S')} (è€—æ—¶: {duration:.1f}ç§’)*")
                    lines.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        all_queries = task_context.get_all_queries()
        if all_queries:
            lines.append("### ğŸ“Š æ€»ä½“ç»Ÿè®¡")
            lines.append("")
            lines.append(f"- **æ€»æŸ¥è¯¢æ•°:** {len(all_queries)}")
            lines.append(f"- **æˆåŠŸæŸ¥è¯¢:** {len(task_context.get_successful_queries())}")
            lines.append(f"- **å¤±è´¥æŸ¥è¯¢:** {len(task_context.get_failed_queries())}")
            
            csv_files = task_context.get_all_csv_files()
            if csv_files:
                total_rows = sum(f["row_count"] for f in csv_files)
                lines.append(f"- **ç”Ÿæˆæ–‡ä»¶:** {len(csv_files)} ä¸ª")
                lines.append(f"- **æ€»æ•°æ®è¡Œæ•°:** {total_rows:,} è¡Œ")
            lines.append("")
        
        return "\n".join(lines)

    async def _execute_engineer_streaming(self, instruction: str, task_id: Optional[str] = None):
        """
        åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ AutoSQLQueryToolï¼ˆå·²åºŸå¼ƒæµå¼æ‰§è¡Œï¼Œæ”¹ä¸ºç›´æ¥è°ƒç”¨ï¼‰

        Args:
            instruction: æŒ‡ä»¤å†…å®¹
            task_id: ä»»åŠ¡ID

        Yields:
            Dict: äº‹ä»¶å­—å…¸ï¼ŒåŒ…å« type å’Œç›¸å…³æ•°æ®
        """
        import asyncio
        from concurrent.futures import ThreadPoolExecutor

        loop = asyncio.get_event_loop()

        def run_query():
            """åœ¨çº¿ç¨‹ä¸­è¿è¡ŒæŸ¥è¯¢"""
            try:
                # ç›´æ¥è°ƒç”¨AutoSQLQueryTool
                date_range = "last_7_days"
                filename = f"task_{task_id}_query.csv" if task_id else None
                
                result = self.agent.auto_sql_query_tool.forward(
                    user_query=instruction,
                    date_range=date_range,
                    filename=filename
                )
                
                # è§£æç»“æœ
                import json
                result_data = json.loads(result)
                
                return {
                    "status": "success",
                    "instruction": instruction,
                    "result": result,
                    "timestamp": datetime.now().isoformat()
                }
            except Exception as e:
                return {
                    "status": "error",
                    "instruction": instruction,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
        executor = ThreadPoolExecutor(max_workers=1)
        future = executor.submit(run_query)

        # ç­‰å¾…ç»“æœ
        result = await loop.run_in_executor(None, future.result)
        
        # è¿”å›ç»“æœ
        yield {
            "type": "result",
            "data": result
        }

        executor.shutdown(wait=False)

    def _extract_sql_and_csv(self, result: Dict[str, Any]) -> Optional[str]:
        """
        ä»æŸ¥è¯¢ç»“æœä¸­æå–SQLè¯­å¥å’ŒCSVæ–‡ä»¶è·¯å¾„

        Args:
            result: æŸ¥è¯¢ç»“æœå­—å…¸

        Returns:
            æ ¼å¼åŒ–çš„SQLå’ŒCSVä¿¡æ¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        import re

        try:
            result_text = result.get("result", "")
            if not result_text:
                return None

            # ç¡®ä¿result_textæ˜¯å­—ç¬¦ä¸²
            if not isinstance(result_text, str):
                result_text = str(result_text)

            info_lines = []

            # æå–SQLè¯­å¥ - ä»ç»“æœæ–‡æœ¬ä¸­æŸ¥æ‰¾SQL
            sql_match = re.search(r'(?:æ‰§è¡ŒSQL|SQLæŸ¥è¯¢|ç”Ÿæˆçš„SQL)[:\s]*\n?```(?:sql)?\s*\n?(.*?)\n?```', result_text, re.DOTALL | re.IGNORECASE)
            if not sql_match:
                # å°è¯•å…¶ä»–æ¨¡å¼
                sql_match = re.search(r'SELECT\s+.*?FROM\s+.*?(?:WHERE|GROUP|ORDER|LIMIT|;|\n\n)', result_text, re.DOTALL | re.IGNORECASE)

            if sql_match:
                sql_text = sql_match.group(1) if sql_match.lastindex else sql_match.group(0)
                sql_text = sql_text.strip()
                # æ¸…ç†SQLæ–‡æœ¬
                sql_text = re.sub(r'\s+', ' ', sql_text)  # å‹ç¼©å¤šä½™ç©ºæ ¼
                if len(sql_text) > 200:
                    sql_text = sql_text[:200] + "..."
                info_lines.append(f"ğŸ“ SQL: {sql_text}")

            # æå–CSVæ–‡ä»¶è·¯å¾„
            csv_match = re.search(r'CSV\s*æ–‡ä»¶[:\s]*(?:\[([^\]]+)\]|\`([^\`]+)\`|([^\n]+))', result_text, re.IGNORECASE)
            if csv_match:
                csv_path = csv_match.group(1) or csv_match.group(2) or csv_match.group(3)
                csv_path = csv_path.strip()
                # æå–æ–‡ä»¶å
                csv_filename = csv_path.split('/')[-1]

                # å¦‚æœæœ‰base_urlï¼Œç”Ÿæˆä¸‹è½½é“¾æ¥
                if hasattr(self.agent, 'base_url') and self.agent.base_url:
                    download_url = f"{self.agent.base_url.rstrip('/')}/files/{csv_filename}"
                    info_lines.append(f"ğŸ’¾ CSVæ–‡ä»¶: {csv_filename}")
                    info_lines.append(f"ğŸ“¥ ä¸‹è½½é“¾æ¥: {download_url}")
                else:
                    info_lines.append(f"ğŸ’¾ CSVæ–‡ä»¶: {csv_path}")

            if info_lines:
                return "\n".join(info_lines) + "\n"

            return None

        except Exception as e:
            logger.warning(f"æå–SQLå’ŒCSVä¿¡æ¯å¤±è´¥: {e}")
            return None


# ============ APIç«¯ç‚¹ ============

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–Agent"""
    global agent, settings

    logger.info("åˆå§‹åŒ–ç¥ç­–æ•°æ®åˆ†æAgent...")
    settings = get_settings()

    try:
        # ä»ç¯å¢ƒå˜é‡è·å–base_urlï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
        base_url = os.getenv("API_BASE_URL", "http://localhost:8000")
        logger.info(f"API Base URL: {base_url}")

        agent = create_agent_v2(base_url=base_url)
        logger.info("Agentåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"Agentåˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    global agent

    if agent:
        logger.info("å…³é—­Agentèµ„æº...")
        agent.close()


@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "service": "ç¥ç­–æ•°æ®åˆ†æåŠ©æ‰‹ API",
        "version": "2.0.0"
    }


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "object": "list",
        "data": [
            {
                "id": "sensors-agent",
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": "sensors-analytics"
            }
        ]
    }


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨

    æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§æ¨¡å¼ï¼š
    - æµå¼(stream=true): å®æ—¶è¿”å›thinkingæ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆ
    - éæµå¼(stream=false): è¿”å›æœ€ç»ˆå®Œæ•´ç­”æ¡ˆ
    """
    global agent

    if not agent:
        raise HTTPException(status_code=503, detail="Agentæœªåˆå§‹åŒ–")

    # æå–ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯
    user_messages = [msg for msg in request.messages if msg.role == "user"]
    if not user_messages:
        raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")

    user_input = user_messages[-1].content

    # ç”Ÿæˆè¯·æ±‚ID
    request_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created_at = int(datetime.now().timestamp())

    # æµå¼å“åº”
    if request.stream:
        async def generate_stream():
            """ç”ŸæˆSSEæµ"""
            wrapper = StreamingAgentWrapper(agent)
            # ä½¿ç”¨request_idä½œä¸ºtask_id
            task_id = request_id.replace("chatcmpl-", "")

            # é¦–å…ˆå‘é€role
            chunk = ChatCompletionStreamResponse(
                id=request_id,
                created=created_at,
                model=request.model,
                choices=[
                    ChatCompletionStreamChoice(
                        index=0,
                        delta=DeltaMessage(role="assistant"),
                        finish_reason=None
                    )
                ]
            )
            yield f"data: {chunk.model_dump_json()}\n\n"

            # æµå¼å¤„ç†æŸ¥è¯¢ï¼Œä¼ é€’task_id
            async for step in wrapper.query_streaming(user_input, task_id=task_id):
                step_type = step.get("type")
                content = step.get("content", "")

                if step_type == "thinking":
                    # å‘é€thinkingæ­¥éª¤
                    chunk = ChatCompletionStreamResponse(
                        id=request_id,
                        created=created_at,
                        model=request.model,
                        choices=[
                            ChatCompletionStreamChoice(
                                index=0,
                                delta=DeltaMessage(content=content),
                                finish_reason=None
                            )
                        ]
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"

                elif step_type == "answer":
                    # å‘é€åˆ†éš”ç¬¦
                    separator = "\n\n" + "=" * 60 + "\n\n"
                    chunk = ChatCompletionStreamResponse(
                        id=request_id,
                        created=created_at,
                        model=request.model,
                        choices=[
                            ChatCompletionStreamChoice(
                                index=0,
                                delta=DeltaMessage(content=separator),
                                finish_reason=None
                            )
                        ]
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"

                    # å‘é€æœ€ç»ˆç­”æ¡ˆ
                    chunk = ChatCompletionStreamResponse(
                        id=request_id,
                        created=created_at,
                        model=request.model,
                        choices=[
                            ChatCompletionStreamChoice(
                                index=0,
                                delta=DeltaMessage(content=content),
                                finish_reason=None
                            )
                        ]
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"

                elif step_type == "error":
                    # å‘é€é”™è¯¯ä¿¡æ¯
                    chunk = ChatCompletionStreamResponse(
                        id=request_id,
                        created=created_at,
                        model=request.model,
                        choices=[
                            ChatCompletionStreamChoice(
                                index=0,
                                delta=DeltaMessage(content=f"\n\nâŒ é”™è¯¯: {content}"),
                                finish_reason="error"
                            )
                        ]
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"

            # å‘é€ç»“æŸæ ‡è®°
            chunk = ChatCompletionStreamResponse(
                id=request_id,
                created=created_at,
                model=request.model,
                choices=[
                    ChatCompletionStreamChoice(
                        index=0,
                        delta=DeltaMessage(),
                        finish_reason="stop"
                    )
                ]
            )
            yield f"data: {chunk.model_dump_json()}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream"
        )

    # éæµå¼å“åº”
    else:
        try:
            # åŒæ­¥è°ƒç”¨Agentï¼Œä¼ é€’task_id
            task_id = request_id.replace("chatcmpl-", "")
            result = agent.query(user_input, task_id=task_id)

            response = ChatCompletionResponse(
                id=request_id,
                created=created_at,
                model=request.model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=Message(role="assistant", content=result),
                        finish_reason="stop"
                    )
                ],
                usage=Usage(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0
                )
            )

            return response

        except Exception as e:
            logger.exception("æŸ¥è¯¢å¤„ç†å¤±è´¥")
            raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")


@app.post("/reset")
async def reset_agent():
    """é‡ç½®Agentå¯¹è¯çŠ¶æ€"""
    global agent

    if not agent:
        raise HTTPException(status_code=503, detail="Agentæœªåˆå§‹åŒ–")

    agent.reset()
    return {"status": "ok", "message": "AgentçŠ¶æ€å·²é‡ç½®"}


@app.get("/files/{filename}")
async def download_file(filename: str):
    """
    ä¸‹è½½CSVæ–‡ä»¶

    Args:
        filename: CSVæ–‡ä»¶å

    Returns:
        æ–‡ä»¶ä¸‹è½½å“åº”

    Example:
        GET /files/refund_events_cdp_tag_fill_rate.csv
    """
    global settings

    # è·å–é…ç½®çš„CSVè¾“å‡ºç›®å½•
    csv_dir = settings.SQL_OUTPUT_DIR if settings else "/tmp/sensors_data"

    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(csv_dir, filename)

    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨å…è®¸çš„ç›®å½•å†…ï¼ˆé˜²æ­¢è·¯å¾„éå†æ”»å‡»ï¼‰
    csv_dir_abs = os.path.abspath(csv_dir)
    file_path_abs = os.path.abspath(file_path)

    if not file_path_abs.startswith(csv_dir_abs):
        logger.warning(f"æ‹’ç»è®¿é—®éæ³•è·¯å¾„: {filename}")
        raise HTTPException(status_code=403, detail="è®¿é—®è¢«æ‹’ç»")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

    # æ£€æŸ¥æ˜¯å¦ä¸ºCSVæ–‡ä»¶
    if not filename.lower().endswith('.csv'):
        logger.warning(f"éCSVæ–‡ä»¶è®¿é—®è¯·æ±‚: {filename}")
        raise HTTPException(status_code=400, detail="åªæ”¯æŒä¸‹è½½CSVæ–‡ä»¶")

    logger.info(f"æä¾›æ–‡ä»¶ä¸‹è½½: {filename}")

    # è¿”å›æ–‡ä»¶å“åº”
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={filename}",
            "Cache-Control": "no-cache"
        }
    )


@app.get("/files")
async def list_files():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„CSVæ–‡ä»¶

    Returns:
        æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«æ–‡ä»¶åã€å¤§å°ã€ä¿®æ”¹æ—¶é—´ç­‰ä¿¡æ¯

    Example:
        GET /files
    """
    global settings

    csv_dir = settings.SQL_OUTPUT_DIR if settings else "/tmp/sensors_data"

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_dir):
        return {"files": [], "message": "è¾“å‡ºç›®å½•ä¸å­˜åœ¨"}

    try:
        files_info = []

        for filename in os.listdir(csv_dir):
            if not filename.endswith('.csv'):
                continue

            file_path = os.path.join(csv_dir, filename)

            # è·å–æ–‡ä»¶ä¿¡æ¯
            stat = os.stat(file_path)

            files_info.append({
                "filename": filename,
                "size_bytes": stat.st_size,
                "size_human": f"{stat.st_size / 1024:.2f} KB",
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "download_url": f"/files/{filename}"
            })

        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åº
        files_info.sort(key=lambda x: x["modified_time"], reverse=True)

        return {
            "files": files_info,
            "total_count": len(files_info),
            "directory": csv_dir
        }

    except Exception as e:
        logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}")


# ============ ä¸»å‡½æ•° ============

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
