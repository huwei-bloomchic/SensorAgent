"""
Agentç¼–æ’å™¨ V2 - åŒå±‚æ¶æ„
ä¸»è¦çš„æ™ºèƒ½ä»£ç†ï¼Œåè°ƒä¸Šå±‚åˆ†æAgentå’Œä¸‹å±‚SQLæ‰§è¡ŒAgent
"""
from typing import Optional, Dict, Any
from loguru import logger
from datetime import datetime
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from config.settings import get_settings
from src.sensors.client import SensorsClient
from src.agents.analyst_agent import AnalystAgent
from src.agents.engineer_agent import EngineerAgent
from src.models.task_context import TaskContext
from src.utils.report_formatter import ReportFormatter


class SensorsAnalyticsAgentV2:
    """
    ç¥ç­–æ•°æ®åˆ†ææ™ºèƒ½åŠ©æ‰‹ V2 - åŒå±‚æ¶æ„

    æ¶æ„:
    - ä¸Šå±‚: AnalystAgent (åˆ†æè§„åˆ’) - æ‡‚ä¸šåŠ¡ï¼Œä¸æ‡‚SQL
    - ä¸‹å±‚: EngineerAgent (SQLæ‰§è¡Œ) - æ‡‚SQLï¼Œä¸æ‡‚ä¸šåŠ¡å½’å› 
    - åè°ƒ: Orchestrator - è´Ÿè´£ä¸Šä¸‹å±‚é€šä¿¡å’Œæµç¨‹æ§åˆ¶

    åŠŸèƒ½:
    - ç†è§£ç”¨æˆ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    - ä¸Šå±‚Agentç”Ÿæˆåˆ†æè®¡åˆ’
    - ä¸‹å±‚Agentæ‰§è¡ŒSQLæŸ¥è¯¢
    - ä¸Šå±‚Agentç»¼åˆç»“æœå¹¶ç”Ÿæˆæ´å¯Ÿ
    """

    def __init__(
        self,
        sensors_client: Optional[SensorsClient] = None,
        analyst_model_name: Optional[str] = None,
        engineer_model_name: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–åŒå±‚Agentæ¶æ„

        Args:
            sensors_client: ç¥ç­–APIå®¢æˆ·ç«¯(å¯é€‰)
            analyst_model_name: ä¸Šå±‚Agentæ¨¡å‹åç§°(å¯é€‰)
            engineer_model_name: ä¸‹å±‚Agentæ¨¡å‹åç§°(å¯é€‰)
            api_key: APIå¯†é’¥(å¯é€‰)
            base_url: APIæœåŠ¡å™¨åŸºç¡€URLï¼Œç”¨äºç”ŸæˆCSVä¸‹è½½é“¾æ¥(å¯é€‰)
        """
        self.settings = get_settings()
        self.base_url = base_url

        # åˆå§‹åŒ–ç¥ç­–å®¢æˆ·ç«¯
        if sensors_client is None:
            sensors_client = self._create_sensors_client()
        self.sensors_client = sensors_client

        # åˆå§‹åŒ–ä¸Šå±‚åˆ†æAgent
        logger.info("åˆå§‹åŒ–ä¸Šå±‚åˆ†æAgent (AnalystAgent)...")
        self.analyst_agent = AnalystAgent(
            model_name=analyst_model_name or self.settings.LITELLM_MODEL,
            api_key=api_key or self.settings.LITELLM_API_KEY
        )

        # åˆå§‹åŒ–ä¸‹å±‚SQLæ‰§è¡ŒAgent
        logger.info("åˆå§‹åŒ–ä¸‹å±‚SQLæ‰§è¡ŒAgent (EngineerAgent)...")
        self.engineer_agent = EngineerAgent(
            sensors_client=sensors_client,
            model_name=engineer_model_name or self.settings.LITELLM_MODEL,
            api_key=api_key or self.settings.LITELLM_API_KEY,
            base_url=base_url
        )

        logger.info("=" * 80)
        logger.info("åŒå±‚Agentæ¶æ„åˆå§‹åŒ–å®Œæˆ")
        logger.info("  â”œâ”€ ä¸Šå±‚: AnalystAgent (ä¸šåŠ¡åˆ†æ)")
        logger.info("  â””â”€ ä¸‹å±‚: EngineerAgent (SQLæ‰§è¡Œ)")
        logger.info("=" * 80)

    def _create_sensors_client(self) -> SensorsClient:
        """åˆ›å»ºç¥ç­–APIå®¢æˆ·ç«¯"""
        logger.info("åˆ›å»ºç¥ç­–APIå®¢æˆ·ç«¯...")

        client = SensorsClient(
            api_url=self.settings.SENSORS_API_URL,
            project=self.settings.SENSORS_PROJECT,
            api_key=self.settings.SENSORS_API_KEY,
            timeout=self.settings.REQUEST_TIMEOUT,
            max_retries=self.settings.MAX_RETRIES
        )

        return client

    def query(
        self,
        user_input: str,
        enable_progressive_analysis: bool = True,
        task_id: Optional[str] = None,
        task_context: Optional[TaskContext] = None
    ) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - æ¸è¿›å¼åŒå±‚æ¶æ„åä½œæµç¨‹

        æ¸è¿›å¼åˆ†ææµç¨‹:
        1. ä¸Šå±‚Agentç”Ÿæˆåˆæ­¥æŸ¥è¯¢æŒ‡ä»¤
        2. ä¸‹å±‚Agentæ‰§è¡Œåˆæ­¥æŸ¥è¯¢
        3. ä¸Šå±‚Agentè¯„ä¼°ç»“æœï¼Œå†³å®šæ˜¯å¦éœ€è¦ä¸‹é’»
        4. å¦‚æœéœ€è¦ï¼Œç”Ÿæˆä¸‹é’»æŒ‡ä»¤å¹¶æ‰§è¡Œ
        5. ç»¼åˆæ‰€æœ‰ç»“æœï¼Œç”Ÿæˆæ´å¯Ÿ

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            enable_progressive_analysis: æ˜¯å¦å¯ç”¨æ¸è¿›å¼åˆ†æ (é»˜è®¤True)
            task_id: ä»»åŠ¡IDï¼Œç”¨äºCSVæ–‡ä»¶å‘½å (å¯é€‰)
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨ï¼Œå¦åˆ™åˆ›å»ºæ–°çš„ (å¯é€‰)

        Returns:
            åˆ†æç»“æœå’Œæ´å¯Ÿ
        """
        # å¦‚æœæ²¡æœ‰æä¾›task_idï¼Œç”Ÿæˆä¸€ä¸ª
        import uuid
        if not task_id:
            task_id = uuid.uuid4().hex[:8]

        # åˆ›å»ºæˆ–ä½¿ç”¨æä¾›çš„TaskContext
        if task_context is None:
            task_context = TaskContext(
                task_id=task_id,
                user_question=user_input
            )
        self.task_context = task_context  # ä¿å­˜ä¸ºå®ä¾‹å˜é‡ï¼Œæ–¹ä¾¿å…¶ä»–æ–¹æ³•è®¿é—®

        logger.info("=" * 80)
        logger.info(f"[Orchestrator V2] å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_input}")
        logger.info(f"[æ¸è¿›å¼åˆ†æ] {'å¯ç”¨' if enable_progressive_analysis else 'ç¦ç”¨'}")
        logger.info(f"[TaskContext] ä»»åŠ¡ID: {task_id}")
        logger.info("=" * 80)

        try:
            # ============ é˜¶æ®µ1: åˆæ­¥åˆ†æå’Œè§„åˆ’ ============
            logger.info("\n" + "=" * 80)
            logger.info("ã€é˜¶æ®µ1ã€‘ä¸Šå±‚åˆ†æAgent - ç”Ÿæˆåˆæ­¥æŸ¥è¯¢")
            logger.info("=" * 80)

            # å¼€å§‹åˆå§‹è¿­ä»£
            task_context.start_iteration(
                iteration_type="initial",
                name="åˆæ­¥æŸ¥è¯¢",
                description="æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆåˆæ­¥æŸ¥è¯¢è®¡åˆ’"
            )

            analysis_result = self.analyst_agent.analyze(user_input, stage="initial")
            analysis_plan = analysis_result.get("analysis_plan", "")

            logger.info(f"[åˆæ­¥åˆ†æè®¡åˆ’]\n{analysis_plan}")

            # è§£æåˆ†æè®¡åˆ’ï¼Œæå–åˆæ­¥æŒ‡ä»¤
            initial_instructions = self._parse_instructions(analysis_plan)

            if not initial_instructions:
                logger.warning("æœªèƒ½ä»åˆ†æè®¡åˆ’ä¸­æå–åˆ°å…·ä½“æŒ‡ä»¤ï¼Œä½¿ç”¨é»˜è®¤æŒ‡ä»¤")
                initial_instructions = [{
                    "task": user_input,
                    "time_range": "last_7_days",
                    "description": "ç›´æ¥æ‰§è¡Œç”¨æˆ·æŸ¥è¯¢"
                }]

            logger.info(f"\næå–åˆ° {len(initial_instructions)} æ¡åˆæ­¥æŸ¥è¯¢æŒ‡ä»¤:")
            for i, inst in enumerate(initial_instructions, 1):
                logger.info(f"  {i}. {inst.get('task', inst)}")

            # ============ é˜¶æ®µ2: æ‰§è¡Œåˆæ­¥æŸ¥è¯¢ ============
            logger.info("\n" + "=" * 80)
            logger.info("ã€é˜¶æ®µ2ã€‘ä¸‹å±‚æ‰§è¡ŒAgent - æ‰§è¡Œåˆæ­¥æŸ¥è¯¢")
            logger.info("=" * 80)

            initial_results = self._execute_instructions(
                initial_instructions,
                task_id=task_id,
                task_context=task_context
            )

            # æ£€æŸ¥åˆæ­¥æŸ¥è¯¢æ˜¯å¦æˆåŠŸ
            success_count = sum(1 for r in initial_results if r.get("status") == "success")
            logger.info(f"\nåˆæ­¥æŸ¥è¯¢å®Œæˆ: {success_count}/{len(initial_results)} æˆåŠŸ")

            # å®Œæˆåˆå§‹è¿­ä»£
            task_context.complete_iteration()

            # ============ é˜¶æ®µ3: è¯„ä¼°æ˜¯å¦éœ€è¦ä¸‹é’» ============
            drilldown_results = []
            drilldown_instructions = []

            if enable_progressive_analysis and success_count > 0:
                logger.info("\n" + "=" * 80)
                logger.info("ã€é˜¶æ®µ3ã€‘ä¸Šå±‚åˆ†æAgent - è¯„ä¼°æ˜¯å¦éœ€è¦ä¸‹é’»")
                logger.info("=" * 80)

                decision = self.analyst_agent.evaluate_and_decide_drilldown(
                    user_question=user_input,
                    initial_results=initial_results
                )

                logger.info(f"\n[ä¸‹é’»å†³ç­–] {'éœ€è¦ä¸‹é’»' if decision['need_drilldown'] else 'ä¸éœ€è¦ä¸‹é’»'}")
                logger.info(f"[å†³ç­–ç†ç”±] {decision['reasoning']}")

                # ============ é˜¶æ®µ4: æ‰§è¡Œä¸‹é’»æŸ¥è¯¢ (å¦‚æœéœ€è¦) ============
                if decision["need_drilldown"]:
                    logger.info("\n" + "=" * 80)
                    logger.info("ã€é˜¶æ®µ4ã€‘ç”Ÿæˆå¹¶æ‰§è¡Œä¸‹é’»æŸ¥è¯¢")
                    logger.info("=" * 80)

                    # å¼€å§‹ä¸‹é’»è¿­ä»£
                    task_context.start_iteration(
                        iteration_type="drilldown",
                        name="æ·±å…¥åˆ†æ",
                        description="åŸºäºåˆæ­¥ç»“æœè¿›è¡Œæ·±å…¥åˆ†æ"
                    )

                    # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
                    context = {
                        "initial_results": self.analyst_agent._extract_results_summary(initial_results),
                        "suggested_dimensions": decision["suggested_dimensions"]
                    }

                    # ç”Ÿæˆä¸‹é’»æŒ‡ä»¤
                    drilldown_analysis = self.analyst_agent.analyze(
                        user_question=user_input,
                        context=context,
                        stage="drilldown"
                    )
                    drilldown_plan = drilldown_analysis.get("analysis_plan", "")
                    drilldown_instructions = self._parse_instructions(drilldown_plan)

                    if drilldown_instructions:
                        logger.info(f"\næå–åˆ° {len(drilldown_instructions)} æ¡ä¸‹é’»æŒ‡ä»¤:")
                        for i, inst in enumerate(drilldown_instructions, 1):
                            logger.info(f"  {i}. {inst.get('task', inst)}")

                        # æ‰§è¡Œä¸‹é’»æŸ¥è¯¢ï¼Œä¼ é€’task_idå’Œtask_context
                        drilldown_results = self._execute_instructions(
                            drilldown_instructions,
                            task_id=task_id,
                            task_context=task_context
                        )

                        # å®Œæˆä¸‹é’»è¿­ä»£
                        task_context.complete_iteration()

                        drilldown_success = sum(1 for r in drilldown_results if r.get("status") == "success")
                        logger.info(f"\nä¸‹é’»æŸ¥è¯¢å®Œæˆ: {drilldown_success}/{len(drilldown_results)} æˆåŠŸ")
                    else:
                        logger.info("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ä¸‹é’»æŒ‡ä»¤")
                else:
                    logger.info("\nåˆæ­¥ç»“æœå·²è¶³å¤Ÿï¼Œè·³è¿‡ä¸‹é’»åˆ†æ")

            # ============ é˜¶æ®µ5: ç»¼åˆåˆ†æ ============
            logger.info("\n" + "=" * 80)
            logger.info("ã€é˜¶æ®µ5ã€‘ä¸Šå±‚åˆ†æAgent - ç»¼åˆæ‰€æœ‰ç»“æœ")
            logger.info("=" * 80)

            # åˆå¹¶æ‰€æœ‰ç»“æœ
            all_results = initial_results + drilldown_results
            all_instructions = initial_instructions + drilldown_instructions

            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            task_context.completed_at = datetime.now()

            # å¦‚æœåªæœ‰ä¸€ä¸ªåˆæ­¥æŸ¥è¯¢ä¸”æˆåŠŸï¼Œä¸”ä¸éœ€è¦ä¸‹é’»ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼è¿”å›
            if len(all_results) == 1 and all_results[0].get("status") == "success" and not drilldown_results:
                logger.info("å•ä¸€æŸ¥è¯¢æˆåŠŸä¸”ä¸éœ€è¦ä¸‹é’»ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼")
                final_result = ReportFormatter.format_single_result(
                    user_question=user_input,
                    result=all_results[0]
                )
                logger.info("=" * 80)
                logger.info("[Orchestrator V2] æŸ¥è¯¢å¤„ç†å®Œæˆ")
                logger.info("=" * 80)
                return final_result

            # éœ€è¦ç»¼åˆåˆ†æ
            logger.info("å¼€å§‹ç»¼åˆå¤šä¸ªæŸ¥è¯¢ç»“æœ...")
            synthesis_report = self.analyst_agent.synthesize_results(
                instructions=all_instructions,
                results=all_results
            )

            # ============ è¿”å›æœ€ç»ˆç»“æœ ============
            logger.info("=" * 80)
            logger.info("[Orchestrator V2] æŸ¥è¯¢å¤„ç†å®Œæˆ")
            logger.info("=" * 80)

            # æ„å»ºæœ€ç»ˆè¾“å‡º
            return ReportFormatter.format_multiple_results(
                user_question=user_input,
                analysis_plan=analysis_plan,
                initial_results=initial_results,
                drilldown_results=drilldown_results,
                synthesis_report=synthesis_report,
                extract_plan_summary=self._extract_plan_summary
            )

        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}"
            logger.error("=" * 80)
            logger.error(f"[Orchestrator V2] {error_msg}")
            logger.error("=" * 80)
            logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            return error_msg

    def _generate_instruction_hash(self, instruction: str) -> str:
        """
        ç”ŸæˆæŒ‡ä»¤çš„å”¯ä¸€æ ‡è¯†hash

        ç”¨äºæŸ¥è¯¢å»é‡ï¼šç›¸åŒçš„æŒ‡ä»¤ä¼šç”Ÿæˆç›¸åŒçš„hash

        Args:
            instruction: æŒ‡ä»¤å­—ç¬¦ä¸²

        Returns:
            32ä½çš„MD5 hashå­—ç¬¦ä¸²
        """
        import hashlib

        # æ ‡å‡†åŒ–æŒ‡ä»¤æ–‡æœ¬ï¼ˆå»é™¤ç©ºæ ¼ã€è½¬å°å†™ï¼‰ä»¥ä¾¿æ›´å¥½åœ°åŒ¹é…
        normalized = instruction.strip().lower()

        # ç”ŸæˆMD5 hash
        hash_obj = hashlib.md5(normalized.encode('utf-8'))
        return hash_obj.hexdigest()

    def _parse_instructions(self, analysis_plan: str) -> list:
        """
        ä»åˆ†æè®¡åˆ’ä¸­è§£ææŒ‡ä»¤

        Args:
            analysis_plan: åˆ†æè®¡åˆ’æ–‡æœ¬

        Returns:
            æŒ‡ä»¤åˆ—è¡¨
        """
        instructions = []

        try:
            # å°è¯•ä»JSONä»£ç å—ä¸­æå–
            json_pattern = r'```(?:json|python)?\s*(\{.*?\})\s*```'
            matches = re.finditer(json_pattern, analysis_plan, re.DOTALL)

            for match in matches:
                try:
                    json_str = match.group(1)
                    # ç§»é™¤Pythonå˜é‡èµ‹å€¼
                    json_str = re.sub(r'^\s*\w+\s*=\s*', '', json_str)
                    instruction = json.loads(json_str)
                    instructions.append(instruction)
                except json.JSONDecodeError:
                    continue

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
            if not instructions:
                # æŸ¥æ‰¾"æŸ¥è¯¢"ã€"åˆ†æ"ç­‰å…³é”®è¯å¼€å¤´çš„è¡Œ
                lines = analysis_plan.split('\n')
                for line in lines:
                    line = line.strip()
                    if any(keyword in line for keyword in ["æŸ¥è¯¢", "åˆ†æ", "ç»Ÿè®¡", "è®¡ç®—"]):
                        if len(line) > 10 and not line.startswith('#'):
                            instructions.append({"task": line})

        except Exception as e:
            logger.error(f"è§£ææŒ‡ä»¤å¤±è´¥: {e}")

        return instructions

    def _extract_plan_summary(self, analysis_plan: str) -> str:
        """
        ä»åˆ†æè®¡åˆ’ä¸­æå–æ‘˜è¦

        Args:
            analysis_plan: åˆ†æè®¡åˆ’æ–‡æœ¬

        Returns:
            è®¡åˆ’æ‘˜è¦
        """
        # æå–å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦
        lines = analysis_plan.split('\n')
        summary_lines = []

        for line in lines:
            line = line.strip()
            # è·³è¿‡ä»£ç å—
            if line.startswith('```') or line.startswith('{') or line.startswith('['):
                continue
            if line:
                summary_lines.append(line)
                if len(summary_lines) >= 5:  # åªè¦å‰5è¡Œéç©ºè¡Œ
                    break

        return '\n'.join(summary_lines) if summary_lines else "è‡ªåŠ¨åˆ†æ"

    def _execute_instructions(
        self,
        instructions: list,
        task_id: Optional[str] = None,
        task_context: Optional[TaskContext] = None,
        max_concurrent: int = 6
    ) -> list:
        """
        æ‰§è¡Œä¸€ç»„æŒ‡ä»¤ï¼Œæ”¯æŒæŸ¥è¯¢å»é‡å’Œç¼“å­˜ï¼Œå¹¶è®°å½•åˆ°TaskContext
        æ”¯æŒå¹¶å‘æ‰§è¡Œï¼Œæœ€å¤šåŒæ—¶æ‰§è¡Œ max_concurrent ä¸ªä»»åŠ¡

        Args:
            instructions: æŒ‡ä»¤åˆ—è¡¨
            task_id: ä»»åŠ¡IDï¼Œç”¨äºCSVæ–‡ä»¶å‘½å (å¯é€‰)
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼Œç”¨äºè®°å½•ä¸­é—´æ•°æ® (å¯é€‰)
            max_concurrent: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼Œé»˜è®¤6

        Returns:
            æ‰§è¡Œç»“æœåˆ—è¡¨ï¼ˆä¿æŒä¸è¾“å…¥æŒ‡ä»¤ç›¸åŒçš„é¡ºåºï¼‰
        """
        if not instructions:
            return []

        # çº¿ç¨‹å®‰å…¨çš„ç¼“å­˜å’Œè®¡æ•°å™¨
        query_cache = {}  # æŸ¥è¯¢ç¼“å­˜: {æŒ‡ä»¤hash: ç»“æœ}
        cache_lock = threading.Lock()
        deduplicated_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

        # é¢„å¤„ç†æ‰€æœ‰æŒ‡ä»¤ï¼Œå‡†å¤‡æ‰§è¡Œå‚æ•°
        execution_tasks = []
        for i, instruction in enumerate(instructions):
            # å°†æŒ‡ä»¤è½¬æ¢ä¸ºå­—ç¬¦ä¸²(å¦‚æœæ˜¯å­—å…¸)
            if isinstance(instruction, dict):
                instruction_str = instruction.get("task", json.dumps(instruction, ensure_ascii=False))
                instruction_params = instruction
            else:
                instruction_str = str(instruction)
                instruction_params = {}

            # ç”ŸæˆæŒ‡ä»¤çš„å”¯ä¸€æ ‡è¯†
            instruction_hash = self._generate_instruction_hash(instruction_str)

            # åœ¨TaskContextä¸­åˆ›å»ºæŸ¥è¯¢è®°å½•
            query_ctx = None
            if task_context and task_context.current_iteration:
                query_ctx = task_context.create_query(
                    instruction=instruction_str,
                    context=None,
                    parameters=instruction_params
                )

            execution_tasks.append({
                "index": i,
                "instruction": instruction,
                "instruction_str": instruction_str,
                "instruction_params": instruction_params,
                "instruction_hash": instruction_hash,
                "query_ctx": query_ctx
            })

        # å®šä¹‰å•ä¸ªæŒ‡ä»¤çš„æ‰§è¡Œå‡½æ•°
        def execute_single_instruction(task_info: dict) -> tuple:
            """æ‰§è¡Œå•ä¸ªæŒ‡ä»¤ï¼Œè¿”å› (ç´¢å¼•, ç»“æœ)"""
            i = task_info["index"]
            instruction_str = task_info["instruction_str"]
            instruction_hash = task_info["instruction_hash"]
            query_ctx = task_info["query_ctx"]

            logger.info(f"\n--- æ‰§è¡ŒæŒ‡ä»¤ {i+1}/{len(instructions)} ---")

            # æ£€æŸ¥ç¼“å­˜ï¼ˆéœ€è¦åŠ é”ï¼‰
            with cache_lock:
                if instruction_hash in query_cache:
                    logger.info(f"âš¡ æ£€æµ‹åˆ°é‡å¤æŒ‡ä»¤ï¼Œä½¿ç”¨ç¼“å­˜ç»“æœ (hash: {instruction_hash[:8]}...)")
                    result = query_cache[instruction_hash].copy()
                    result["from_cache"] = True
                    if query_ctx:
                        query_ctx.from_cache = True
                    deduplicated_count[0] += 1
                    return (i, result)

            # æ‰§è¡Œæ–°æŒ‡ä»¤ï¼ˆä¸åœ¨é”å†…æ‰§è¡Œï¼Œé¿å…é˜»å¡å…¶ä»–ä»»åŠ¡ï¼‰
            logger.info(f"ğŸ” æ‰§è¡Œæ–°æŒ‡ä»¤ (hash: {instruction_hash[:8]}...)")
            result = self.engineer_agent.execute_instruction(
                instruction_str,
                context=None,
                task_id=task_id
            )
            result["query_hash"] = instruction_hash
            result["instruction"] = instruction_str

            # è®°å½•ç»“æœåˆ°TaskContext
            if query_ctx:
                self._record_result_to_context(query_ctx, result)

            # ç¼“å­˜æˆåŠŸçš„æŸ¥è¯¢ç»“æœï¼ˆéœ€è¦åŠ é”ï¼‰
            if result.get("status") == "success":
                with cache_lock:
                    query_cache[instruction_hash] = result.copy()

            return (i, result)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        execution_results = [None] * len(instructions)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨ï¼Œä¿æŒé¡ºåº

        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(instructions)} ä¸ªæŒ‡ä»¤ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrent}")

        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(execute_single_instruction, task): task
                for task in execution_tasks
            }

            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_task):
                try:
                    index, result = future.result()
                    execution_results[index] = result

                    completed_count += 1
                    status = result.get("status")
                    cache_info = " (ç¼“å­˜)" if result.get("from_cache") else ""
                    
                    if status == "success":
                        logger.info(f"âœ… æŒ‡ä»¤ {index+1}/{len(instructions)} æ‰§è¡ŒæˆåŠŸ{cache_info} [{completed_count}/{len(instructions)}]")
                    elif status == "partial":
                        logger.warning(f"âš ï¸  æŒ‡ä»¤ {index+1}/{len(instructions)} éƒ¨åˆ†å®Œæˆ: {result.get('result', result.get('error'))} [{completed_count}/{len(instructions)}]")
                    else:
                        logger.error(f"âŒ æŒ‡ä»¤ {index+1}/{len(instructions)} æ‰§è¡Œå¤±è´¥: {result.get('error')} [{completed_count}/{len(instructions)}]")
                except Exception as e:
                    # å¤„ç†æ‰§è¡Œå¼‚å¸¸
                    task = future_to_task[future]
                    logger.exception(f"âŒ æŒ‡ä»¤ {task['index']+1} æ‰§è¡Œå¼‚å¸¸: {e}")
                    execution_results[task["index"]] = {
                        "status": "error",
                        "instruction": task["instruction_str"],
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    }

        # è®°å½•å»é‡ç»Ÿè®¡
        if deduplicated_count[0] > 0:
            logger.info(f"\nğŸ’¾ æŸ¥è¯¢å»é‡: é¿å…äº† {deduplicated_count[0]} æ¬¡é‡å¤æ‰§è¡Œ")

        logger.info(f"âœ¨ æ‰€æœ‰æŒ‡ä»¤æ‰§è¡Œå®Œæˆ: {len(execution_results)} ä¸ªç»“æœ")

        return execution_results

    def _record_result_to_context(self, query_ctx: Any, result: Dict[str, Any]):
        """
        å°†æŸ¥è¯¢ç»“æœè®°å½•åˆ°TaskContext

        Args:
            query_ctx: QueryContextå¯¹è±¡
            result: æŸ¥è¯¢ç»“æœå­—å…¸
        """
        try:
            # è§£æresultä¸­çš„JSONæ•°æ®
            result_data = result.get("result", "")
            if isinstance(result_data, str):
                import json
                try:
                    result_data = json.loads(result_data)
                except:
                    pass

            # è®°å½•SQL
            if isinstance(result_data, dict):
                sql = result_data.get("sql_executed") or result_data.get("sql")
                if sql:
                    query_ctx.set_sql(sql)
                    query_ctx.mark_sql_executed()

                # è®°å½•CSVæ•°æ®
                csv_path = result_data.get("csv_path")
                if csv_path:
                    query_ctx.set_data(
                        csv_path=csv_path,
                        row_count=result_data.get("rows", 0),
                        column_count=result_data.get("column_count"),
                        columns=result_data.get("columns"),
                        data_preview=result_data.get("data_preview", []),
                        download_url=result_data.get("download_url")
                    )

            # è®°å½•çŠ¶æ€
            status = result.get("status", "unknown")
            error = result.get("error")
            query_ctx.complete(status=status, error=error)

        except Exception as e:
            logger.warning(f"è®°å½•ç»“æœåˆ°ä¸Šä¸‹æ–‡å¤±è´¥: {e}")

    def get_task_context(self) -> Optional[TaskContext]:
        """
        è·å–å½“å‰ä»»åŠ¡çš„TaskContext

        Returns:
            TaskContextå¯¹è±¡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return getattr(self, 'task_context', None)


    def reset(self):
        """é‡ç½®å¯¹è¯çŠ¶æ€"""
        logger.info("é‡ç½®åŒå±‚AgentçŠ¶æ€")
        # é‡æ–°åˆå§‹åŒ–ä¸¤ä¸ªAgent
        self.analyst_agent = AnalystAgent(
            model_name=self.settings.LITELLM_MODEL,
            api_key=self.settings.LITELLM_API_KEY
        )
        self.engineer_agent = EngineerAgent(
            sensors_client=self.sensors_client,
            model_name=self.settings.LITELLM_MODEL,
            api_key=self.settings.LITELLM_API_KEY
        )

    def close(self):
        """å…³é—­èµ„æº"""
        logger.info("å…³é—­åŒå±‚Agentèµ„æº")
        if self.engineer_agent:
            self.engineer_agent.close()
        if self.sensors_client:
            self.sensors_client.close()


def create_agent_v2(
    analyst_model_name: Optional[str] = None,
    engineer_model_name: Optional[str] = None,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None
) -> SensorsAnalyticsAgentV2:
    """
    å·¥å‚å‡½æ•°: åˆ›å»ºåŒå±‚æ¶æ„çš„ç¥ç­–åˆ†æAgent

    Args:
        analyst_model_name: ä¸Šå±‚Agentæ¨¡å‹åç§°
        engineer_model_name: ä¸‹å±‚Agentæ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        base_url: APIæœåŠ¡å™¨åŸºç¡€URLï¼Œç”¨äºç”ŸæˆCSVä¸‹è½½é“¾æ¥

    Returns:
        SensorsAnalyticsAgentV2å®ä¾‹
    """
    return SensorsAnalyticsAgentV2(
        analyst_model_name=analyst_model_name,
        engineer_model_name=engineer_model_name,
        api_key=api_key,
        base_url=base_url
    )
