"""
æ•°æ®åˆ†æå·¥å…·

å¯¹æŸ¥è¯¢ç»“æœè¿›è¡Œæ·±åº¦æ•°æ®åˆ†æï¼Œç”Ÿæˆè¶‹åŠ¿ã€å¼‚å¸¸å’Œç»Ÿè®¡æ´å¯Ÿ
"""

from typing import List, Optional, Dict, Any
from smolagents import Tool
from loguru import logger
import json

from src.analysis import TrendAnalyzer, StatisticsAnalyzer, AnomalyDetector, InsightGenerator
from src.analysis.utils import (
    parse_data_to_dataframe,
    extract_structured_data,
    prepare_timeseries_data,
    validate_analysis_params,
    detect_data_quality_issues,
    calculate_confidence_level,
    sample_large_dataset
)


class DataAnalysisTool(Tool):
    """
    æ•°æ®åˆ†æå·¥å…·

    å¯¹æŸ¥è¯¢ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬è¶‹åŠ¿åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€ç»Ÿè®¡åˆ†æç­‰
    """

    name = "analyze_data"

    description = """å¯¹æŸ¥è¯¢ç»“æœè¿›è¡Œæ·±åº¦æ•°æ®åˆ†æï¼Œç”Ÿæˆè¶‹åŠ¿ã€å¼‚å¸¸å’Œç»Ÿè®¡æ´å¯Ÿã€‚

    ã€ä½¿ç”¨åœºæ™¯ã€‘
    - ç”¨æˆ·è¯¢é—®"è¶‹åŠ¿"ã€"å˜åŒ–"ã€"å¢é•¿"ã€"ä¸‹é™" â†’ ä½¿ç”¨trendåˆ†æ
    - ç”¨æˆ·è¯¢é—®"å¼‚å¸¸"ã€"çªç„¶"ã€"æ³¢åŠ¨"ã€"å³°å€¼" â†’ ä½¿ç”¨anomalyæ£€æµ‹
    - ç”¨æˆ·è¯¢é—®"ç»Ÿè®¡"ã€"å¹³å‡"ã€"åˆ†å¸ƒ" â†’ ä½¿ç”¨statisticsåˆ†æ
    - ç”¨æˆ·è¯¢é—®"å¯¹æ¯”"ã€"å·®å¼‚" â†’ ä½¿ç”¨comparisonåˆ†æ

    ã€è¾“å…¥æ ¼å¼ã€‘
    data: JSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼Œåº”åŒ…å«columnså’Œrowså­—æ®µ
    ä¾‹å¦‚ï¼š{"columns": ["date", "count"], "rows": [["2024-01-01", 100], ["2024-01-02", 120]]}

    ä¹Ÿå¯ä»¥ç›´æ¥ä»SQLQueryToolçš„è¿”å›ç»“æœä¸­æå–<structured_data>æ ‡ç­¾å†…çš„JSONæ•°æ®ã€‚

    analysis_types: åˆ†æç±»å‹åˆ—è¡¨ï¼Œå¯é€‰å€¼ï¼š
    - "trend": è¶‹åŠ¿åˆ†æï¼ˆå¢é•¿ç‡ã€ç§»åŠ¨å¹³å‡ã€æ‹ç‚¹ï¼‰
    - "anomaly": å¼‚å¸¸æ£€æµ‹ï¼ˆçªå˜ç‚¹ã€å¼‚å¸¸å€¼ï¼‰
    - "statistics": ç»Ÿè®¡åˆ†æï¼ˆå‡å€¼ã€åˆ†ä½æ•°ã€åˆ†å¸ƒï¼‰

    metric_columns: è¦åˆ†æçš„æŒ‡æ ‡åˆ—ååˆ—è¡¨ï¼ˆå¿…å¡«ï¼‰
    time_column: æ—¶é—´åˆ—åï¼ˆå¯é€‰ï¼Œç”¨äºæ—¶é—´åºåˆ—åˆ†æï¼‰
    context: ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°ï¼ˆå¯é€‰ï¼Œå¸®åŠ©ç”Ÿæˆæ›´å‡†ç¡®çš„æ´å¯Ÿï¼‰

    ã€è¾“å‡ºã€‘
    ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š
    - è¶‹åŠ¿æ–¹å‘ã€å¢é•¿ç‡ã€å³°å€¼/è°·å€¼ã€å‘¨æœŸæ€§æ¨¡å¼
    - å¼‚å¸¸ç‚¹ã€çªå˜ç‚¹
    - ç»Ÿè®¡æŒ‡æ ‡ã€åˆ†å¸ƒç‰¹å¾
    - å…³é”®æ´å¯Ÿå’Œå¯è¡Œå»ºè®®

    ã€ç¤ºä¾‹ã€‘
    analyze_data(
        data='{"columns": ["date", "purchases"], "rows": [["2024-11-01", 1234], ["2024-11-02", 1456], ...]}',
        analysis_types=["trend", "anomaly"],
        metric_columns=["purchases"],
        time_column="date",
        context="30å¤©è´­ä¹°è¶‹åŠ¿åˆ†æ"
    )
    """

    inputs = {
        "data": {
            "type": "string",
            "description": "JSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼ŒåŒ…å«columnså’Œrowså­—æ®µ"
        },
        "analysis_types": {
            "type": "array",
            "description": "åˆ†æç±»å‹åˆ—è¡¨ï¼Œå¯é€‰: trend, anomaly, statistics"
        },
        "metric_columns": {
            "type": "array",
            "description": "è¦åˆ†æçš„æŒ‡æ ‡åˆ—ååˆ—è¡¨"
        },
        "time_column": {
            "type": "string",
            "description": "æ—¶é—´åˆ—åï¼ˆå¯é€‰ï¼‰",
            "nullable": True
        },
        "context": {
            "type": "string",
            "description": "ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°ï¼ˆå¯é€‰ï¼‰",
            "nullable": True
        }
    }

    output_type = "string"

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åˆ†æå·¥å…·"""
        super().__init__()

        # åˆå§‹åŒ–å„ä¸ªåˆ†æå™¨
        self.trend_analyzer = TrendAnalyzer()
        self.statistics_analyzer = StatisticsAnalyzer()
        self.anomaly_detector = AnomalyDetector()
        self.insight_generator = InsightGenerator()

        logger.info("DataAnalysisTool åˆå§‹åŒ–å®Œæˆ")

    def forward(
        self,
        data: str,
        analysis_types: List[str],
        metric_columns: List[str],
        time_column: Optional[str] = None,
        context: Optional[str] = None
    ) -> str:
        """
        æ‰§è¡Œæ•°æ®åˆ†æ

        Args:
            data: JSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®
            analysis_types: åˆ†æç±»å‹åˆ—è¡¨
            metric_columns: è¦åˆ†æçš„æŒ‡æ ‡åˆ—ååˆ—è¡¨
            time_column: æ—¶é—´åˆ—å
            context: ä¸šåŠ¡ä¸Šä¸‹æ–‡

        Returns:
            åˆ†ææŠ¥å‘Šï¼ˆæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼‰
        """
        try:
            logger.info(f"å¼€å§‹æ•°æ®åˆ†æï¼Œåˆ†æç±»å‹: {analysis_types}, æŒ‡æ ‡åˆ—: {metric_columns}")

            # 1. å‚æ•°éªŒè¯
            valid_types = ["trend", "anomaly", "statistics"]
            is_valid, error_msg = validate_analysis_params(data, analysis_types, valid_types)
            if not is_valid:
                return f"âŒ å‚æ•°éªŒè¯å¤±è´¥: {error_msg}"

            # 2. è§£ææ•°æ®
            # é¦–å…ˆå°è¯•æå–<structured_data>æ ‡ç­¾
            structured_data = extract_structured_data(data)
            if structured_data:
                data = json.dumps(structured_data)

            df = parse_data_to_dataframe(data)

            if len(df) == 0:
                return "âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ"

            logger.info(f"è§£ææ•°æ®æˆåŠŸï¼Œè¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")

            # 3. æ•°æ®è´¨é‡æ£€æŸ¥
            quality_report = detect_data_quality_issues(df)
            if quality_report['has_issues']:
                logger.warning(f"æ•°æ®è´¨é‡é—®é¢˜: {quality_report['issues']}")

            # 4. é‡‡æ ·å¤§æ•°æ®é›†
            if len(df) > 1000:
                logger.info(f"æ•°æ®é‡è¾ƒå¤§({len(df)}è¡Œ)ï¼Œè¿›è¡Œé‡‡æ ·åˆ†æ")
                original_size = len(df)
                df = sample_large_dataset(df, max_rows=1000)
                logger.info(f"é‡‡æ ·åæ•°æ®é‡: {len(df)}è¡Œ")

            # 5. å‡†å¤‡æ•°æ®
            all_results = {}

            # å¯¹æ¯ä¸ªæŒ‡æ ‡åˆ—è¿›è¡Œåˆ†æ
            for metric_column in metric_columns:
                if metric_column not in df.columns:
                    logger.warning(f"æŒ‡æ ‡åˆ—ä¸å­˜åœ¨: {metric_column}")
                    continue

                logger.info(f"åˆ†ææŒ‡æ ‡: {metric_column}")

                # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
                try:
                    value_series, time_series = prepare_timeseries_data(
                        df, time_column, metric_column
                    )
                except Exception as e:
                    logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
                    return f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}"

                # 6. æ‰§è¡Œå„ç±»åˆ†æ
                metric_results = {}

                # è¶‹åŠ¿åˆ†æ
                if "trend" in analysis_types:
                    logger.info("æ‰§è¡Œè¶‹åŠ¿åˆ†æ...")
                    try:
                        trend_result = self.trend_analyzer.comprehensive_analysis(
                            value_series,
                            time_index=time_series,
                            metric_name=metric_column
                        )
                        metric_results["trend_analysis"] = trend_result
                    except Exception as e:
                        logger.error(f"è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
                        metric_results["trend_analysis"] = {"error": str(e)}

                # å¼‚å¸¸æ£€æµ‹
                if "anomaly" in analysis_types:
                    logger.info("æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
                    try:
                        anomaly_result = self.anomaly_detector.comprehensive_detection(
                            value_series,
                            methods=["zscore", "iqr", "sudden_change"],
                            time_index=time_series
                        )
                        metric_results["anomaly_detection"] = anomaly_result
                    except Exception as e:
                        logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
                        metric_results["anomaly_detection"] = {"error": str(e)}

                # ç»Ÿè®¡åˆ†æ
                if "statistics" in analysis_types:
                    logger.info("æ‰§è¡Œç»Ÿè®¡åˆ†æ...")
                    try:
                        stats_result = self.statistics_analyzer.comprehensive_analysis(
                            value_series,
                            metric_name=metric_column
                        )
                        metric_results["statistics"] = stats_result
                    except Exception as e:
                        logger.error(f"ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}")
                        metric_results["statistics"] = {"error": str(e)}

                all_results[metric_column] = metric_results

            # 7. ç”Ÿæˆæ´å¯Ÿ
            logger.info("ç”Ÿæˆåˆ†ææ´å¯Ÿ...")

            # 8. æ ¼å¼åŒ–è¾“å‡º
            output = self._format_analysis_report(
                all_results,
                analysis_types,
                context,
                quality_report,
                len(df)
            )

            logger.info("æ•°æ®åˆ†æå®Œæˆ")
            return output

        except Exception as e:
            error_msg = f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return f"âŒ {error_msg}"

    def _format_analysis_report(
        self,
        results: Dict[str, Any],
        analysis_types: List[str],
        context: Optional[str],
        quality_report: Dict[str, Any],
        data_size: int
    ) -> str:
        """
        æ ¼å¼åŒ–åˆ†ææŠ¥å‘Š

        Args:
            results: åˆ†æç»“æœ
            analysis_types: åˆ†æç±»å‹
            context: ä¸šåŠ¡ä¸Šä¸‹æ–‡
            quality_report: æ•°æ®è´¨é‡æŠ¥å‘Š
            data_size: æ•°æ®é‡

        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        lines = []

        # æ ‡é¢˜
        lines.append("=" * 60)
        lines.append("ğŸ“Š æ•°æ®åˆ†ææŠ¥å‘Š")
        if context:
            lines.append(f"åˆ†æå¯¹è±¡: {context}")
        lines.append("=" * 60)
        lines.append("")

        # æ•°æ®æ¦‚è§ˆ
        lines.append("ã€æ•°æ®æ¦‚è§ˆã€‘")
        lines.append(f"  æ•°æ®é‡: {data_size} è¡Œ")
        lines.append(f"  åˆ†ææŒ‡æ ‡: {', '.join(results.keys())}")
        lines.append(f"  åˆ†æç±»å‹: {', '.join(analysis_types)}")

        # ç½®ä¿¡åº¦
        confidence = calculate_confidence_level(data_size, "general")
        lines.append(f"  åˆ†æç½®ä¿¡åº¦: {confidence}")

        # æ•°æ®è´¨é‡è­¦å‘Š
        if quality_report.get('warnings'):
            for warning in quality_report['warnings']:
                lines.append(f"  âš ï¸  {warning}")

        lines.append("")

        # éå†æ¯ä¸ªæŒ‡æ ‡çš„åˆ†æç»“æœ
        for metric_name, metric_results in results.items():
            lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            lines.append(f"æŒ‡æ ‡: {metric_name}")
            lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            lines.append("")

            # è¶‹åŠ¿åˆ†æç»“æœ
            if "trend_analysis" in metric_results:
                trend_result = metric_results["trend_analysis"]

                if "error" not in trend_result:
                    lines.append("ã€è¶‹åŠ¿åˆ†æã€‘")

                    # æ•´ä½“è¶‹åŠ¿
                    if "trend" in trend_result:
                        trend = trend_result["trend"]
                        lines.append(f"  æ•´ä½“è¶‹åŠ¿: {trend.get('description', 'æœªçŸ¥')} {trend.get('emoji', '')}")

                        if "growth" in trend_result:
                            growth = trend_result["growth"]
                            lines.append(f"  {growth.get('description', '')}")

                            if "daily_growth_rate_pct" in growth:
                                lines.append(f"  æ—¥å‡å¢é•¿ç‡: {growth['daily_growth_rate_pct']:.2f}%")

                    # å³°å€¼/è°·å€¼
                    if "turning_points" in trend_result:
                        tp = trend_result["turning_points"]

                        if tp.get("max_peak"):
                            peak = tp["max_peak"]
                            lines.append(f"  ğŸ“ˆ å³°å€¼: ç¬¬ {peak['index']} ä¸ªç‚¹, å€¼: {peak['value']:.2f}")

                        if tp.get("min_trough"):
                            trough = tp["min_trough"]
                            lines.append(f"  ğŸ“‰ è°·å€¼: ç¬¬ {trough['index']} ä¸ªç‚¹, å€¼: {trough['value']:.2f}")

                    # å‘¨æœŸæ€§
                    if "periodicity" in trend_result:
                        periodicity = trend_result["periodicity"]
                        if periodicity.get("has_periodicity"):
                            lines.append(f"  ğŸ”„ å‘¨æœŸæ€§: {periodicity.get('description', '')}")

                    lines.append("")

            # å¼‚å¸¸æ£€æµ‹ç»“æœ
            if "anomaly_detection" in metric_results:
                anomaly_result = metric_results["anomaly_detection"]

                if "error" not in anomaly_result:
                    lines.append("ã€å¼‚å¸¸æ£€æµ‹ã€‘")

                    if "summary" in anomaly_result:
                        summary = anomaly_result["summary"]
                        total_anomalies = summary.get("total_anomaly_points", 0)

                        if total_anomalies > 0:
                            lines.append(f"  æ£€æµ‹åˆ° {total_anomalies} ä¸ªå¼‚å¸¸æ•°æ®ç‚¹")

                            # æ˜¾ç¤ºå„æ–¹æ³•çš„æ£€æµ‹ç»“æœ
                            results_by_method = anomaly_result.get("results_by_method", {})

                            # Z-scoreå¼‚å¸¸
                            if "zscore" in results_by_method:
                                zscore = results_by_method["zscore"]
                                anomalies = zscore.get("anomalies", [])
                                if len(anomalies) > 0:
                                    top = anomalies[0]
                                    lines.append(f"  âš ï¸  ç»Ÿè®¡å¼‚å¸¸: ç¬¬ {top['index']} ä¸ªç‚¹åç¦» {abs(top['deviation']):.1f}%")

                            # çªå˜æ£€æµ‹
                            if "sudden_change" in results_by_method:
                                changes = results_by_method["sudden_change"].get("changes", [])
                                if len(changes) > 0:
                                    top = changes[0]
                                    change_type = "æ¿€å¢" if top['type'] == 'surge' else "éª¤é™"
                                    lines.append(f"  âš ï¸  çªå˜ç‚¹: ç¬¬ {top['index']} ä¸ªç‚¹{change_type} {abs(top['change_rate_pct']):.1f}%")

                        else:
                            lines.append("  âœ“ æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸")

                    lines.append("")

            # ç»Ÿè®¡åˆ†æç»“æœ
            if "statistics" in metric_results:
                stats_result = metric_results["statistics"]

                if "error" not in stats_result and "basic_stats" in stats_result:
                    lines.append("ã€ç»Ÿè®¡åˆ†æã€‘")
                    stats = stats_result["basic_stats"]

                    lines.append(f"  å¹³å‡å€¼: {stats.get('mean', 0):.2f}")
                    lines.append(f"  ä¸­ä½æ•°: {stats.get('median', 0):.2f}")
                    lines.append(f"  æ ‡å‡†å·®: {stats.get('std', 0):.2f}")
                    lines.append(f"  èŒƒå›´: [{stats.get('min', 0):.2f}, {stats.get('max', 0):.2f}]")

                    quantiles = stats.get('quantiles', {})
                    lines.append(f"  åˆ†ä½æ•° (25%/50%/75%): {quantiles.get('q25', 0):.2f} / {quantiles.get('q50', 0):.2f} / {quantiles.get('q75', 0):.2f}")

                    lines.append("")

        # ç”Ÿæˆç»¼åˆæ´å¯Ÿ
        lines.append("ã€ç»¼åˆæ´å¯Ÿã€‘")

        # æå–æ‰€æœ‰æŒ‡æ ‡çš„æ´å¯Ÿ
        all_insights = []
        for metric_name, metric_results in results.items():
            if "trend_analysis" in metric_results:
                trend_insights = self.insight_generator.generate_trend_insights(metric_results["trend_analysis"])
                all_insights.extend(trend_insights)

            if "anomaly_detection" in metric_results:
                anomaly_insights = self.insight_generator.generate_anomaly_insights(metric_results["anomaly_detection"])
                all_insights.extend(anomaly_insights)

        # æ˜¾ç¤ºé«˜ä¼˜å…ˆçº§æ´å¯Ÿ
        high_priority_insights = [i for i in all_insights if i.get('priority') == 'high']
        if high_priority_insights:
            for insight in high_priority_insights[:5]:  # æœ€å¤šæ˜¾ç¤º5æ¡
                lines.append(f"  â€¢ {insight['insight']}")
        else:
            lines.append("  æ•°æ®è¡¨ç°æ­£å¸¸ï¼Œæœªå‘ç°æ˜¾è‘—å¼‚å¸¸æˆ–è¶‹åŠ¿å˜åŒ–")

        lines.append("")

        # è¡ŒåŠ¨å»ºè®®
        recommendations = self.insight_generator.generate_recommendations(all_insights, context)
        if recommendations:
            lines.append("ã€è¡ŒåŠ¨å»ºè®®ã€‘")
            for i, rec in enumerate(recommendations[:5], 1):
                lines.append(f"  {i}. {rec}")
            lines.append("")

        lines.append("=" * 60)

        # é™„åŠ ç»“æ„åŒ–æ•°æ®ä¾›LLMè§£è¯»
        lines.append("")
        lines.append("<analysis_insights>")
        insight_json = self.insight_generator.format_for_llm(
            {metric: results[metric] for metric in results.keys()},
            context
        )
        lines.append(insight_json)
        lines.append("</analysis_insights>")

        return "\n".join(lines)
