"""
SQLæ‰§è¡Œå·¥å…·
æ‰§è¡Œç¥ç­–SQLæŸ¥è¯¢å¹¶å°†ç»“æœè½¬æ¢ä¸ºCSVæ–‡ä»¶
"""
import os
import json
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from pathlib import Path

import pandas as pd
from smolagents import Tool
from loguru import logger

from config.settings import get_settings


class SQLExecutionTool(Tool):
    """
    SQLæ‰§è¡Œå’ŒCSVè½¬æ¢å·¥å…·

    æ‰§è¡Œç¥ç­–SQLæŸ¥è¯¢ï¼Œå°†æµå¼JSONLå“åº”è½¬æ¢ä¸ºCSVæ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°
    """

    name = "sql_execution"

    description = """æ‰§è¡Œç¥ç­–SQLæŸ¥è¯¢å¹¶å°†ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶ã€‚

ä½¿ç”¨æ­¤å·¥å…·å¯ä»¥ï¼š
- æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è·å–ç»“æœ
- å°†ç»“æœè‡ªåŠ¨è½¬æ¢ä¸ºCSVæ ¼å¼
- ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
- è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ç»“æœï¼ŒåŒ…å«CSVæ–‡ä»¶è·¯å¾„ã€æ•°æ®æ‘˜è¦å’Œé¢„è§ˆ

å‚æ•°è¯´æ˜ï¼š
- sql: SQLæŸ¥è¯¢è¯­å¥ï¼ˆå¿…å¡«ï¼‰
- output_dir: CSVè¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
- filename: CSVæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰

è¿”å›å€¼ï¼š
è¿”å›ä¸€ä¸ªæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«ï¼š
- CSVæ–‡ä»¶è·¯å¾„
- æ•°æ®è¡Œæ•°å’Œåˆ—ä¿¡æ¯
- æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰
- <structured_data>æ ‡ç­¾å†…çš„JSONæ•°æ®ï¼ˆåŒ…å«csv_pathã€rowsã€columnsç­‰ç»“æ„åŒ–ä¿¡æ¯ï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
result = sql_execution(
    sql="SELECT date, COUNT(*) as count FROM events WHERE event='ProductClick' AND date BETWEEN '2024-12-01' AND '2024-12-07' GROUP BY date",
    filename="product_clicks.csv"
)
# result æ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯
# å¯ä»¥ä» <structured_data> æ ‡ç­¾ä¸­æå–ç»“æ„åŒ–æ•°æ®

æ³¨æ„ï¼š
- è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
- è‡ªåŠ¨æ¸…ç†è¶…è¿‡24å°æ—¶çš„æ—§CSVæ–‡ä»¶
- è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å…ƒç»„ï¼ä¸è¦å°è¯•è§£åŒ…ï¼
- å¦‚éœ€æå–CSVè·¯å¾„ï¼Œè¯·ä»è¿”å›å­—ç¬¦ä¸²çš„ <structured_data> éƒ¨åˆ†è§£æJSON
"""

    inputs = {
        "sql": {
            "type": "string",
            "description": "è¦æ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥"
        },
        "output_dir": {
            "type": "string",
            "description": "CSVè¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰",
            "nullable": True
        },
        "filename": {
            "type": "string",
            "description": "CSVæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰",
            "nullable": True
        }
    }

    output_type = "string"

    def __init__(self, sensors_client):
        """
        åˆå§‹åŒ–SQLæ‰§è¡Œå·¥å…·

        Args:
            sensors_client: ç¥ç­–APIå®¢æˆ·ç«¯
        """
        super().__init__()
        self.client = sensors_client
        self.settings = get_settings()

        # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
        self.default_output_dir = self.settings.SQL_OUTPUT_DIR if hasattr(self.settings, 'SQL_OUTPUT_DIR') else "/tmp/sensors_data"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self._ensure_output_dir(self.default_output_dir)

        logger.info(f"SQLExecutionTool åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.default_output_dir}")

    def _ensure_output_dir(self, directory: str):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.debug(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {directory}")
        except Exception as e:
            logger.error(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {directory}, é”™è¯¯: {e}")
            raise

    def _generate_filename(self, sql: str) -> str:
        """
        ç”ŸæˆCSVæ–‡ä»¶å

        æ ¼å¼: query_{timestamp}_{hash}.csv

        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥

        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶å
        """
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç”ŸæˆSQLçš„hashï¼ˆå–å‰8ä½ï¼‰
        sql_hash = hashlib.md5(sql.encode()).hexdigest()[:8]

        filename = f"query_{timestamp}_{sql_hash}.csv"
        logger.debug(f"ç”Ÿæˆæ–‡ä»¶å: {filename}")

        return filename

    def _result_to_dataframe(self, result: Dict[str, Any]) -> pd.DataFrame:
        """
        å°†ç¥ç­–APIè¿”å›çš„ç»“æœè½¬æ¢ä¸ºpandas DataFrame

        Args:
            result: APIè¿”å›çš„ç»“æœå­—å…¸

        Returns:
            pandas DataFrame
        """
        # æå–åˆ—åå’Œè¡Œæ•°æ®
        # ç¥ç­–APIå¯èƒ½è¿”å›ä¸¤ç§æ ¼å¼:
        # 1. {'columns': [...], 'rows': [[...], [...]]}  (æ ‡å‡†JSONLç»„åˆæ ¼å¼)
        # 2. {'columns': [...], 'data': [[...], [...]]}  (v3 APIæ ¼å¼)
        columns = result.get('columns', [])

        # å°è¯•ä»'rows'æˆ–'data'å­—æ®µè·å–æ•°æ®
        rows = result.get('rows')
        if rows is None:
            rows = result.get('data', [])

        # å¦‚æœdataæ˜¯å•è¡Œæ•°æ®ï¼ˆä¸æ˜¯åˆ—è¡¨çš„åˆ—è¡¨ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºäºŒç»´æ•°ç»„
        if rows and isinstance(rows, list) and len(rows) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å•è¡Œæ•°æ® (ä¾‹å¦‚: [161611.0] è€Œä¸æ˜¯ [[161611.0]])
            if not isinstance(rows[0], list):
                logger.debug(f"æ£€æµ‹åˆ°å•è¡Œæ•°æ®æ ¼å¼ï¼Œè½¬æ¢ä¸ºäºŒç»´æ•°ç»„: {rows}")
                rows = [rows]  # å°† [161611.0] è½¬æ¢ä¸º [[161611.0]]

        if not columns:
            logger.warning("ç»“æœä¸­æ²¡æœ‰åˆ—åä¿¡æ¯")
            # å¦‚æœæ²¡æœ‰åˆ—åï¼Œå°è¯•ä»ç¬¬ä¸€è¡Œæ¨æ–­
            if rows and len(rows) > 0:
                columns = [f"col_{i}" for i in range(len(rows[0]))]
            else:
                raise ValueError("æ— æ³•åˆ›å»ºDataFrame: ç¼ºå°‘åˆ—ä¿¡æ¯ä¸”æ•°æ®ä¸ºç©º")

        # åˆ›å»ºDataFrame
        try:
            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"æˆåŠŸåˆ›å»ºDataFrame: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
            return df
        except Exception as e:
            logger.error(f"åˆ›å»ºDataFrameå¤±è´¥: {e}")
            logger.error(f"æ•°æ®ç»“æ„: columns={columns}, rows={rows}")
            raise ValueError(f"æ•°æ®æ ¼å¼é”™è¯¯ï¼Œæ— æ³•åˆ›å»ºDataFrame: {str(e)}")

    def _save_csv(self, df: pd.DataFrame, output_path: str) -> str:
        """
        ä¿å­˜DataFrameä¸ºCSVæ–‡ä»¶

        Args:
            df: pandas DataFrame
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            df.to_csv(output_path, index=False, encoding='utf-8')
            file_size = os.path.getsize(output_path)
            logger.info(f"CSVæ–‡ä»¶å·²ä¿å­˜: {output_path}, å¤§å°: {file_size} å­—èŠ‚")
            return output_path
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {output_path}, é”™è¯¯: {e}")
            # å°è¯•å¤‡ç”¨ä½ç½®
            backup_path = f"/tmp/{os.path.basename(output_path)}"
            try:
                df.to_csv(backup_path, index=False, encoding='utf-8')
                logger.warning(f"å·²ä¿å­˜åˆ°å¤‡ç”¨ä½ç½®: {backup_path}")
                return backup_path
            except Exception as e2:
                logger.error(f"å¤‡ç”¨ä½ç½®ä¹Ÿä¿å­˜å¤±è´¥: {e2}")
                raise ValueError(f"æ— æ³•ä¿å­˜CSVæ–‡ä»¶: {str(e)}")

    def _cleanup_old_files(self, directory: str, hours: int = 24):
        """
        æ¸…ç†æ—§çš„CSVæ–‡ä»¶

        Args:
            directory: è¦æ¸…ç†çš„ç›®å½•
            hours: æ–‡ä»¶ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            removed_count = 0

            for filename in os.listdir(directory):
                if not filename.endswith('.csv'):
                    continue

                filepath = os.path.join(directory, filename)

                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))

                if file_mtime < cutoff_time:
                    try:
                        os.remove(filepath)
                        removed_count += 1
                        logger.debug(f"å·²åˆ é™¤æ—§æ–‡ä»¶: {filename}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")

            if removed_count > 0:
                logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {removed_count} ä¸ªè¶…è¿‡ {hours} å°æ—¶çš„CSVæ–‡ä»¶")
        except Exception as e:
            logger.warning(f"æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def _format_result(self, csv_path: str, df: pd.DataFrame, raw_result: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è¾“å‡ºç»“æœ

        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            df: DataFrame
            raw_result: åŸå§‹APIç»“æœ

        Returns:
            æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
        """
        lines = ["=" * 60]
        lines.append("SQL æŸ¥è¯¢æ‰§è¡Œå®Œæˆ")
        lines.append("=" * 60)
        lines.append("")

        # CSVæ–‡ä»¶ä¿¡æ¯
        lines.append(f"âœ… CSV æ–‡ä»¶: {csv_path}")
        lines.append(f"ğŸ“Š è¡Œæ•°: {len(df)}")
        lines.append(f"ğŸ“‹ åˆ—: {list(df.columns)}")
        lines.append("")

        # æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰
        if len(df) > 0:
            lines.append("æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:")
            lines.append("-" * 60)

            # æ ¼å¼åŒ–æ˜¾ç¤ºå‰10è¡Œ
            preview_df = df.head(10)
            preview_str = preview_df.to_string(index=False)
            lines.append(preview_str)

            if len(df) > 10:
                lines.append("")
                lines.append(f"... è¿˜æœ‰ {len(df) - 10} è¡Œæœªæ˜¾ç¤º")
        else:
            lines.append("âš ï¸ æŸ¥è¯¢ç»“æœä¸ºç©º")

        lines.append("")
        lines.append("=" * 60)

        # æ·»åŠ ç»“æ„åŒ–æ•°æ®ä¾›åç»­å¤„ç†
        structured_data = {
            "csv_path": csv_path,
            "rows": len(df),
            "columns": list(df.columns)
        }

        # å°è¯•æå–æ—¥æœŸèŒƒå›´
        if 'date' in df.columns and len(df) > 0:
            try:
                dates = df['date'].dropna().tolist()
                if dates:
                    structured_data["date_range"] = [min(dates), max(dates)]
            except:
                pass

        # æ·»åŠ åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        if len(df) > 0:
            summary_stats = {}
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    try:
                        summary_stats[col] = {
                            "mean": float(df[col].mean()),
                            "min": float(df[col].min()),
                            "max": float(df[col].max()),
                            "sum": float(df[col].sum())
                        }
                    except:
                        pass

            if summary_stats:
                structured_data["summary_stats"] = summary_stats

        lines.append("")
        lines.append("<structured_data>")
        lines.append(json.dumps(structured_data, ensure_ascii=False, indent=2))
        lines.append("</structured_data>")

        return "\n".join(lines)

    def forward(self, sql: str, output_dir: Optional[str] = None, filename: Optional[str] = None) -> str:
        """
        æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶ä¿å­˜ä¸ºCSV

        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰

        Returns:
            æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²ï¼ŒåŒ…å«CSVè·¯å¾„å’Œæ•°æ®æ‘˜è¦
        """
        import time
        tool_start_time = time.time()

        logger.info("=" * 60)
        logger.info("[SQLExecutionTool] å¼€å§‹æ‰§è¡ŒSQLæŸ¥è¯¢")
        logger.info("=" * 60)
        logger.info(f"[SQLæŸ¥è¯¢]\n{sql}")
        logger.info("-" * 60)

        try:
            # 1. æ‰§è¡ŒSQLæŸ¥è¯¢
            step_start = time.time()
            logger.info("[æ­¥éª¤ 1/5] æ‰§è¡ŒSQLæŸ¥è¯¢...")
            result = self.client.execute_sql(sql)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 1/5] âœ“ SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ (APIè€—æ—¶: {step_elapsed:.2f}ç§’)")

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if "error" in result:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")
                raise ValueError(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")

            # 2. è½¬æ¢ä¸ºDataFrame
            step_start = time.time()
            logger.info("[æ­¥éª¤ 2/5] è½¬æ¢æ•°æ®ä¸ºDataFrame...")
            df = self._result_to_dataframe(result)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 2/5] âœ“ DataFrameåˆ›å»ºæˆåŠŸ: {len(df)} è¡Œ x {len(df.columns)} åˆ— (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 3. ç¡®å®šè¾“å‡ºè·¯å¾„
            step_start = time.time()
            logger.info("[æ­¥éª¤ 3/5] ç¡®å®šè¾“å‡ºè·¯å¾„...")
            output_directory = output_dir if output_dir else self.default_output_dir
            self._ensure_output_dir(output_directory)

            if not filename:
                filename = self._generate_filename(sql)

            if not filename.endswith('.csv'):
                filename += '.csv'

            csv_path = os.path.join(output_directory, filename)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 3/5] âœ“ è¾“å‡ºè·¯å¾„: {csv_path} (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 4. ä¿å­˜CSV
            step_start = time.time()
            logger.info("[æ­¥éª¤ 4/5] ä¿å­˜CSVæ–‡ä»¶...")
            csv_path = self._save_csv(df, csv_path)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 4/5] âœ“ CSVæ–‡ä»¶å·²ä¿å­˜ (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 5. æ¸…ç†æ—§æ–‡ä»¶
            step_start = time.time()
            logger.info("[æ­¥éª¤ 5/5] æ¸…ç†æ—§æ–‡ä»¶...")
            cleanup_hours = getattr(self.settings, 'CSV_CLEANUP_HOURS', 24)
            self._cleanup_old_files(output_directory, hours=cleanup_hours)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 5/5] âœ“ æ¸…ç†å®Œæˆ (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 6. æ ¼å¼åŒ–è¿”å›ç»“æœ
            output = self._format_result(csv_path, df, result)

            tool_elapsed = time.time() - tool_start_time
            logger.info("=" * 60)
            logger.info(f"[SQLExecutionTool] æ‰§è¡Œå®Œæˆ (æ€»è€—æ—¶: {tool_elapsed:.2f}ç§’)")
            logger.info("=" * 60)
            return output

        except Exception as e:
            error_msg = f"SQLæ‰§è¡Œæˆ–CSVè½¬æ¢å¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­æ‰§è¡Œæµç¨‹
            raise RuntimeError(error_msg) from e
