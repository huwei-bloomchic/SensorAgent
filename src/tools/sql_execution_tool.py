"""
SQLæ‰§è¡Œå·¥å…·
æ‰§è¡Œç¥ç­–SQLæŸ¥è¯¢å¹¶å°†ç»“æœè½¬æ¢ä¸ºCSVæ–‡ä»¶
"""
import os
import json
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from pathlib import Path

import pandas as pd
from smolagents import Tool
from loguru import logger

from config.settings import get_settings


class SQLExecutionTool(Tool):
    """
    SQLæ‰§è¡Œå’ŒCSVè½¬æ¢å·¥å…·

    æ‰§è¡Œç¥ç­–SQLæŸ¥è¯¢ï¼Œå°†æµå¼JSONLå“åº”è½¬æ¢ä¸ºCSVæ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°
    """

    name = "sql_execution"

    description = """æ‰§è¡Œç¥ç­–SQLæŸ¥è¯¢å¹¶å°†ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶ã€‚

ä½¿ç”¨æ­¤å·¥å…·å¯ä»¥ï¼š
- æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è·å–ç»“æœ
- å°†ç»“æœè‡ªåŠ¨è½¬æ¢ä¸ºCSVæ ¼å¼
- ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
- è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ç»“æœï¼ŒåŒ…å«CSVæ–‡ä»¶è·¯å¾„ã€æ•°æ®æ‘˜è¦å’Œé¢„è§ˆ

å‚æ•°è¯´æ˜ï¼š
- sql: SQLæŸ¥è¯¢è¯­å¥ï¼ˆå¿…å¡«ï¼‰
- output_dir: CSVè¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
- filename: CSVæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰

è¿”å›å€¼ï¼š
è¿”å›ä¸€ä¸ªæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«ï¼š
- CSVæ–‡ä»¶è·¯å¾„
- æ•°æ®è¡Œæ•°å’Œåˆ—ä¿¡æ¯
- æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰
- <structured_data>æ ‡ç­¾å†…çš„JSONæ•°æ®ï¼ˆåŒ…å«csv_pathã€rowsã€columnsç­‰ç»“æ„åŒ–ä¿¡æ¯ï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
result = sql_execution(
    sql="SELECT date, COUNT(*) as count FROM events WHERE event='ProductClick' AND date BETWEEN '2024-12-01' AND '2024-12-07' GROUP BY date",
    filename="product_clicks.csv"
)
# result æ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯
# å¯ä»¥ä» <structured_data> æ ‡ç­¾ä¸­æå–ç»“æ„åŒ–æ•°æ®

æ³¨æ„ï¼š
- è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
- è‡ªåŠ¨æ¸…ç†è¶…è¿‡24å°æ—¶çš„æ—§CSVæ–‡ä»¶
- è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å…ƒç»„ï¼ä¸è¦å°è¯•è§£åŒ…ï¼
- å¦‚éœ€æå–CSVè·¯å¾„ï¼Œè¯·ä»è¿”å›å­—ç¬¦ä¸²çš„ <structured_data> éƒ¨åˆ†è§£æJSON
"""

    inputs = {
        "sql": {
            "type": "string",
            "description": "è¦æ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥"
        },
        "output_dir": {
            "type": "string",
            "description": "CSVè¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰",
            "nullable": True
        },
        "filename": {
            "type": "string",
            "description": "CSVæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰",
            "nullable": True
        }
    }

    output_type = "string"

    def __init__(self, sensors_client, base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–SQLæ‰§è¡Œå·¥å…·

        Args:
            sensors_client: ç¥ç­–APIå®¢æˆ·ç«¯
            base_url: APIæœåŠ¡å™¨çš„åŸºç¡€URLï¼Œç”¨äºç”Ÿæˆä¸‹è½½é“¾æ¥ï¼ˆå¯é€‰ï¼‰
                     ä¾‹å¦‚: "http://localhost:8000" æˆ– "https://api.example.com"
        """
        super().__init__()
        self.client = sensors_client
        self.settings = get_settings()
        self.base_url = base_url

        # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
        self.default_output_dir = self.settings.SQL_OUTPUT_DIR if hasattr(self.settings, 'SQL_OUTPUT_DIR') else "/tmp/sensors_data"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self._ensure_output_dir(self.default_output_dir)

        logger.info(f"SQLExecutionTool åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.default_output_dir}")
        if self.base_url:
            logger.info(f"æ–‡ä»¶ä¸‹è½½URLåŸºç¡€: {self.base_url}")

    def _ensure_output_dir(self, directory: str):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.debug(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {directory}")
        except Exception as e:
            logger.error(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {directory}, é”™è¯¯: {e}")
            raise

    def _generate_filename(self, sql: str) -> str:
        """
        ç”ŸæˆCSVæ–‡ä»¶å

        æ ¼å¼: query_{timestamp}_{hash}.csv

        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥

        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶å
        """
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç”ŸæˆSQLçš„hashï¼ˆå–å‰8ä½ï¼‰
        sql_hash = hashlib.md5(sql.encode()).hexdigest()[:8]

        filename = f"query_{timestamp}_{sql_hash}.csv"
        logger.debug(f"ç”Ÿæˆæ–‡ä»¶å: {filename}")

        return filename

    def _result_to_dataframe(self, result: Dict[str, Any]) -> pd.DataFrame:
        """
        å°†ç¥ç­–APIè¿”å›çš„ç»“æœè½¬æ¢ä¸ºpandas DataFrame

        Args:
            result: APIè¿”å›çš„ç»“æœå­—å…¸

        Returns:
            pandas DataFrame
        """
        # æå–åˆ—åå’Œè¡Œæ•°æ®
        # ç¥ç­–APIå¯èƒ½è¿”å›ä¸¤ç§æ ¼å¼:
        # 1. {'columns': [...], 'rows': [[...], [...]]}  (æ ‡å‡†JSONLç»„åˆæ ¼å¼)
        # 2. {'columns': [...], 'data': [[...], [...]]}  (v3 APIæ ¼å¼)
        columns = result.get('columns', [])

        # å°è¯•ä»'rows'æˆ–'data'å­—æ®µè·å–æ•°æ®
        rows = result.get('rows')
        if rows is None:
            rows = result.get('data', [])

        # å¦‚æœdataæ˜¯å•è¡Œæ•°æ®ï¼ˆä¸æ˜¯åˆ—è¡¨çš„åˆ—è¡¨ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºäºŒç»´æ•°ç»„
        if rows and isinstance(rows, list) and len(rows) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å•è¡Œæ•°æ® (ä¾‹å¦‚: [161611.0] è€Œä¸æ˜¯ [[161611.0]])
            if not isinstance(rows[0], list):
                logger.debug(f"æ£€æµ‹åˆ°å•è¡Œæ•°æ®æ ¼å¼ï¼Œè½¬æ¢ä¸ºäºŒç»´æ•°ç»„: {rows}")
                rows = [rows]  # å°† [161611.0] è½¬æ¢ä¸º [[161611.0]]

        # æ£€æµ‹å¹¶ä¿®å¤åˆ—æ•°ä¸ä¸€è‡´çš„é—®é¢˜
        if rows and isinstance(rows, list) and len(rows) > 0:
            # ç»Ÿè®¡æ¯è¡Œçš„åˆ—æ•°
            row_lengths = [len(row) if isinstance(row, list) else 1 for row in rows]
            max_cols = max(row_lengths)
            min_cols = min(row_lengths)

            if max_cols != min_cols:
                logger.warning(f"æ£€æµ‹åˆ°æ•°æ®è¡Œåˆ—æ•°ä¸ä¸€è‡´: æœ€å°{min_cols}åˆ—, æœ€å¤§{max_cols}åˆ—")
                logger.warning(f"å‰5è¡Œæ•°æ®: {rows[:5]}")

                # å¦‚æœcolumnsæ•°é‡ä¸æœ€å¤§åˆ—æ•°ä¸åŒ¹é…ï¼Œé‡æ–°æ¨æ–­columns
                if len(columns) != max_cols:
                    logger.warning(f"åˆ—åæ•°é‡({len(columns)})ä¸å®é™…æ•°æ®åˆ—æ•°({max_cols})ä¸åŒ¹é…ï¼Œé‡æ–°æ¨æ–­åˆ—å")
                    # ä»æ•°æ®ä¸­æ¨æ–­å®é™…çš„åˆ—æ•°
                    columns = [f"col_{i}" for i in range(max_cols)]

                # å¡«å……è¾ƒçŸ­çš„è¡Œï¼Œä½¿æ‰€æœ‰è¡Œé•¿åº¦ä¸€è‡´
                normalized_rows = []
                for row in rows:
                    if isinstance(row, list):
                        if len(row) < max_cols:
                            # ç”¨Noneå¡«å……ç¼ºå¤±çš„åˆ—
                            normalized_row = row + [None] * (max_cols - len(row))
                            normalized_rows.append(normalized_row)
                        else:
                            normalized_rows.append(row)
                    else:
                        # å•ä¸ªå€¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å¹¶å¡«å……
                        normalized_rows.append([row] + [None] * (max_cols - 1))

                rows = normalized_rows
                logger.info(f"å·²æ ‡å‡†åŒ–æ•°æ®è¡Œï¼Œæ‰€æœ‰è¡Œç°åœ¨éƒ½æœ‰{max_cols}åˆ—")

        if not columns:
            logger.warning("ç»“æœä¸­æ²¡æœ‰åˆ—åä¿¡æ¯")
            # å¦‚æœæ²¡æœ‰åˆ—åï¼Œå°è¯•ä»ç¬¬ä¸€è¡Œæ¨æ–­
            if rows and len(rows) > 0:
                columns = [f"col_{i}" for i in range(len(rows[0]))]
            else:
                raise ValueError("æ— æ³•åˆ›å»ºDataFrame: ç¼ºå°‘åˆ—ä¿¡æ¯ä¸”æ•°æ®ä¸ºç©º")

        # åˆ›å»ºDataFrame
        try:
            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"æˆåŠŸåˆ›å»ºDataFrame: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
            return df
        except Exception as e:
            logger.error(f"åˆ›å»ºDataFrameå¤±è´¥: {e}")
            logger.error(f"æ•°æ®ç»“æ„: columns={columns}, rows={rows[:5] if rows else []}")
            raise ValueError(f"æ•°æ®æ ¼å¼é”™è¯¯ï¼Œæ— æ³•åˆ›å»ºDataFrame: {str(e)}")

    def _save_csv(self, df: pd.DataFrame, output_path: str) -> str:
        """
        ä¿å­˜DataFrameä¸ºCSVæ–‡ä»¶

        Args:
            df: pandas DataFrame
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            df.to_csv(output_path, index=False, encoding='utf-8')
            file_size = os.path.getsize(output_path)
            logger.info(f"CSVæ–‡ä»¶å·²ä¿å­˜: {output_path}, å¤§å°: {file_size} å­—èŠ‚")
            return output_path
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {output_path}, é”™è¯¯: {e}")
            # å°è¯•å¤‡ç”¨ä½ç½®
            backup_path = f"/tmp/{os.path.basename(output_path)}"
            try:
                df.to_csv(backup_path, index=False, encoding='utf-8')
                logger.warning(f"å·²ä¿å­˜åˆ°å¤‡ç”¨ä½ç½®: {backup_path}")
                return backup_path
            except Exception as e2:
                logger.error(f"å¤‡ç”¨ä½ç½®ä¹Ÿä¿å­˜å¤±è´¥: {e2}")
                raise ValueError(f"æ— æ³•ä¿å­˜CSVæ–‡ä»¶: {str(e)}")

    def _cleanup_old_files(self, directory: str, hours: int = 24):
        """
        æ¸…ç†æ—§çš„CSVæ–‡ä»¶

        Args:
            directory: è¦æ¸…ç†çš„ç›®å½•
            hours: æ–‡ä»¶ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            removed_count = 0

            for filename in os.listdir(directory):
                if not filename.endswith('.csv'):
                    continue

                filepath = os.path.join(directory, filename)

                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))

                if file_mtime < cutoff_time:
                    try:
                        os.remove(filepath)
                        removed_count += 1
                        logger.debug(f"å·²åˆ é™¤æ—§æ–‡ä»¶: {filename}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")

            if removed_count > 0:
                logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {removed_count} ä¸ªè¶…è¿‡ {hours} å°æ—¶çš„CSVæ–‡ä»¶")
        except Exception as e:
            logger.warning(f"æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def _format_result(self, csv_path: str, df: pd.DataFrame, raw_result: Dict[str, Any], sql: str = "") -> str:
        """
        æ ¼å¼åŒ–è¾“å‡ºç»“æœï¼Œè¿”å›JSONæ ¼å¼çš„å­—ç¬¦ä¸²

        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            df: DataFrame
            raw_result: åŸå§‹APIç»“æœ
            sql: æ‰§è¡Œçš„SQLè¯­å¥

        Returns:
            JSONæ ¼å¼çš„ç»“æœå­—ç¬¦ä¸²
        """
        csv_filename = os.path.basename(csv_path)

        # å¦‚æœé…ç½®äº†base_urlï¼Œç”ŸæˆHTTPä¸‹è½½é“¾æ¥
        if self.base_url:
            download_url = f"{self.base_url.rstrip('/')}/files/{csv_filename}"
        else:
            download_url = f"file://{csv_path}"

        # æ„å»ºç»“æ„åŒ–æ•°æ®
        result_data = {
            "status": "success",
            "task_id": self._extract_task_id_from_filename(csv_filename),
            "csv_path": csv_path,
            "download_url": download_url,
            "rows": len(df),
            "columns": list(df.columns),
        }

        # æå–æŸ¥è¯¢ä¿¡æ¯
        query_info = {}

        # å°è¯•æå–æ—¥æœŸèŒƒå›´
        if 'date' in df.columns and len(df) > 0:
            try:
                dates = df['date'].dropna().tolist()
                if dates:
                    min_date = min(dates)
                    max_date = max(dates)
                    query_info["date_range"] = f"{min_date} åˆ° {max_date}"
            except:
                pass

        # å°è¯•ä»SQLæå–äº‹ä»¶ä¿¡æ¯
        if sql:
            import re
            # æå–äº‹ä»¶åç§°
            event_match = re.findall(r"event\s*(?:=|IN)\s*\(?'([^']+)'", sql, re.IGNORECASE)
            if event_match:
                query_info["events_analyzed"] = event_match

        # ç»Ÿè®¡æ€»è®°å½•æ•°
        if len(df) > 0:
            query_info["total_records"] = len(df)

        if query_info:
            result_data["query_info"] = query_info

        # ç”Ÿæˆæ•°æ®é¢„è§ˆï¼ˆåˆ†ç»„å±•ç¤ºï¼‰
        data_preview = {}
        if len(df) > 0:
            # æ£€æŸ¥æ˜¯å¦æœ‰äº‹ä»¶åˆ†ç»„
            if 'event' in df.columns or 'äº‹ä»¶åç§°' in df.columns:
                event_col = 'event' if 'event' in df.columns else 'äº‹ä»¶åç§°'

                # æŒ‰äº‹ä»¶å’Œå¹³å°åˆ†ç»„
                if 'web_platform_type' in df.columns or 'å¹³å°ç±»å‹' in df.columns:
                    platform_col = 'web_platform_type' if 'web_platform_type' in df.columns else 'å¹³å°ç±»å‹'

                    for event in df[event_col].unique():
                        event_data = df[df[event_col] == event]
                        event_preview = {}

                        for platform in event_data[platform_col].unique():
                            platform_data = event_data[event_data[platform_col] == platform]

                            # æå–å…³é”®æŒ‡æ ‡
                            preview_item = {}
                            if 'æ€»è®°å½•æ•°' in platform_data.columns:
                                preview_item['total'] = int(platform_data['æ€»è®°å½•æ•°'].iloc[0])

                            # æå–å¡«å……ç‡ä¿¡æ¯
                            for col in platform_data.columns:
                                if 'å¡«å……ç‡' in col:
                                    preview_item[col.replace('%', '')] = f"{platform_data[col].iloc[0]}%"

                            event_preview[str(platform)] = preview_item

                        data_preview[str(event)] = event_preview
                else:
                    # åªæœ‰äº‹ä»¶åˆ†ç»„ï¼Œæ²¡æœ‰å¹³å°
                    for event in df[event_col].unique():
                        event_data = df[df[event_col] == event]
                        preview_item = {}

                        if 'æ€»è®°å½•æ•°' in event_data.columns:
                            preview_item['total'] = int(event_data['æ€»è®°å½•æ•°'].iloc[0])

                        for col in event_data.columns:
                            if 'å¡«å……ç‡' in col:
                                preview_item[col.replace('%', '')] = f"{event_data[col].iloc[0]}%"

                        data_preview[str(event)] = preview_item

        if data_preview:
            result_data["data_preview"] = data_preview

        # ç”Ÿæˆå…³é”®å‘ç°
        key_findings = []
        if len(df) > 0:
            # æ ¹æ®æ•°æ®ç‰¹ç‚¹è‡ªåŠ¨ç”Ÿæˆå…³é”®å‘ç°
            if 'event' in df.columns or 'äº‹ä»¶åç§°' in df.columns:
                event_col = 'event' if 'event' in df.columns else 'äº‹ä»¶åç§°'
                key_findings.append(f"ğŸ“Š åˆ†æäº† {df[event_col].nunique()} ä¸ªäº‹ä»¶ï¼Œå…± {len(df)} æ¡è®°å½•")

            # æ£€æŸ¥å¡«å……ç‡å­—æ®µ
            fill_rate_cols = [col for col in df.columns if 'å¡«å……ç‡' in col]
            if fill_rate_cols:
                for col in fill_rate_cols:
                    try:
                        avg_rate = df[col].mean()
                        key_findings.append(f"ğŸ“ˆ {col}å¹³å‡å€¼: {avg_rate:.2f}%")
                    except:
                        pass

        if key_findings:
            result_data["key_findings"] = key_findings

        # æ·»åŠ æ‰§è¡Œçš„SQL
        if sql:
            result_data["sql_executed"] = sql

        # è¿”å›JSONå­—ç¬¦ä¸²
        return json.dumps(result_data, ensure_ascii=False, indent=2)

    def _extract_task_id_from_filename(self, filename: str) -> str:
        """
        ä»æ–‡ä»¶åä¸­æå–task_id

        Args:
            filename: æ–‡ä»¶åï¼Œä¾‹å¦‚ "task_b72c690f_cdp_tag_fill_rate.csv"

        Returns:
            task_idï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        import re
        match = re.match(r'task_([a-f0-9]+)_', filename)
        if match:
            return match.group(1)
        return ""

    def forward(self, sql: str, output_dir: Optional[str] = None, filename: Optional[str] = None) -> str:
        """
        æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶ä¿å­˜ä¸ºCSV

        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰

        Returns:
            æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²ï¼ŒåŒ…å«CSVè·¯å¾„å’Œæ•°æ®æ‘˜è¦
        """
        import time
        tool_start_time = time.time()

        logger.info("=" * 60)
        logger.info("[SQLExecutionTool] å¼€å§‹æ‰§è¡ŒSQLæŸ¥è¯¢")
        logger.info("=" * 60)
        logger.info(f"[SQLæŸ¥è¯¢]\n{sql}")
        logger.info("-" * 60)

        try:
            # 1. æ‰§è¡ŒSQLæŸ¥è¯¢
            step_start = time.time()
            logger.info("[æ­¥éª¤ 1/5] æ‰§è¡ŒSQLæŸ¥è¯¢...")
            result = self.client.execute_sql(sql)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 1/5] âœ“ SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ (APIè€—æ—¶: {step_elapsed:.2f}ç§’)")

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if "error" in result:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")
                raise ValueError(f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}")

            # 2. è½¬æ¢ä¸ºDataFrame
            step_start = time.time()
            logger.info("[æ­¥éª¤ 2/5] è½¬æ¢æ•°æ®ä¸ºDataFrame...")
            df = self._result_to_dataframe(result)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 2/5] âœ“ DataFrameåˆ›å»ºæˆåŠŸ: {len(df)} è¡Œ x {len(df.columns)} åˆ— (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 3. ç¡®å®šè¾“å‡ºè·¯å¾„
            step_start = time.time()
            logger.info("[æ­¥éª¤ 3/5] ç¡®å®šè¾“å‡ºè·¯å¾„...")
            output_directory = output_dir if output_dir else self.default_output_dir
            self._ensure_output_dir(output_directory)

            if not filename:
                filename = self._generate_filename(sql)

            if not filename.endswith('.csv'):
                filename += '.csv'

            csv_path = os.path.join(output_directory, filename)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 3/5] âœ“ è¾“å‡ºè·¯å¾„: {csv_path} (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 4. ä¿å­˜CSV
            step_start = time.time()
            logger.info("[æ­¥éª¤ 4/5] ä¿å­˜CSVæ–‡ä»¶...")
            csv_path = self._save_csv(df, csv_path)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 4/5] âœ“ CSVæ–‡ä»¶å·²ä¿å­˜ (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 5. æ¸…ç†æ—§æ–‡ä»¶
            step_start = time.time()
            logger.info("[æ­¥éª¤ 5/5] æ¸…ç†æ—§æ–‡ä»¶...")
            cleanup_hours = getattr(self.settings, 'CSV_CLEANUP_HOURS', 24)
            self._cleanup_old_files(output_directory, hours=cleanup_hours)
            step_elapsed = time.time() - step_start
            logger.info(f"[æ­¥éª¤ 5/5] âœ“ æ¸…ç†å®Œæˆ (è€—æ—¶: {step_elapsed:.2f}ç§’)")

            # 6. æ ¼å¼åŒ–è¿”å›ç»“æœ
            output = self._format_result(csv_path, df, result, sql=sql)

            tool_elapsed = time.time() - tool_start_time
            logger.info("=" * 60)
            logger.info(f"[SQLExecutionTool] æ‰§è¡Œå®Œæˆ (æ€»è€—æ—¶: {tool_elapsed:.2f}ç§’)")
            logger.info("=" * 60)
            return output

        except Exception as e:
            error_msg = f"SQLæ‰§è¡Œæˆ–CSVè½¬æ¢å¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­æ‰§è¡Œæµç¨‹
            raise RuntimeError(error_msg) from e
